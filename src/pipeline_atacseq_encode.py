##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
from ruffus.ruffus_utility import add_inputs
"""===========================
Pipeline template
===========================

:Author: Ian Sudbery and Jaime Alvarez-Benayas
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

TODO:
-At the moment sample assigned fraction is using 5' extended single ends, in theory, we should use 5' 1bp filtered single ends. 



Changelog (Rerun any data before these dates):

-08/05/2018: Shouldn't affect anything apart from the output name from the task plot_heatmap_with_dendrograms which
is now name.png and not name.png.png
If the file isn't provided, the task should execute in the same way
Implemented logic to supply a sample_id - add_label file (pipeline.ini [bootstrap_clustering] samples_labels_file
 and add "add_label" text to each sample_id in the bootstrap clustering for tasks:
-bootstrap_cluster_normalized_overlap_sample_with_common_peaks
-bootstrap_cluster_normalized_overlap_sample_with_common_windows (uses bootstrap_cluster_normalized_overlap_sample_with_common_peaks)
-bootstrap_cluster_binarized_sample_broad_narrow_peaks_overlap_with_common_windows (uses bootstrap_cluster_normalized_overlap_sample_with_common_peaks)
-plot_heatmap_with_dendrograms: Corrected name.png.png -> name.png in the output file
Also added the task "plot_heatmap_with_dendrograms" to "typical_clustering_tasks" so that everything is executed as once.



-10/04/2018: Shouldn't affect anything
call_peaks_broad and call_peaks_narrow decorators modified to generate all the files generated by calling the peaks
(not only the .xls files).
The tasks have been adapted internally to these changes.
There are no tasks referencing directly call_peaks_broad and call_peaks_narrow as input tasks,
but the following tasks pull data generated by them through wildcards and have been tested to produce the same results:
filter_peaks, calculateNumberOfPeaks, bdg2tdf_igv, sort_peaks_igv, index_bed_like_igv, generateFCBigWigs, generateLogPvalBigWigs
Additionally, the pipeline pipeline_atac_consensus_balanced_peaks.py is using these methods and has also been adapted and tested.



-19/03/2018: call_peaks_broad is not generating now treat_pileup.bdg and control_lambda.bdg
These two files were already generated in call_peaks_narrow using the additional --SPMR flag which
normalizes fragment pileup per million reads and it is comparable among different samples. Note that
bdg2tdf_igv has been getting only the files generated by call_peaks_narrow all along.


-28/02/2018: Shouldn't affect anything, just avoid unnecessary executions.
Defined a dummy task typical_clustering_tasks which only executes clustering_all_samples_overlap_common_peaks, 
    clustering_all_samples_overlap_common_windows


-09/02/2018: Shouldn't affect anything, just avoid unnecessary executions.
Divide remove_features_before_clusterings into:
    remove_features_sample_with_common_peaks_before_clusterings
    remove_features_sample_with_common_windows_before_clusterings
    remove_features_sample_with_peaks_overlap_sample_with_common_windows_before_clusterings 
    To avoid unnecessary executions of one clustering when calling other clusterings.


-05/02/2018: Affects only other pipelines calling this pipeline.
-call_peaks_broad and call_peaks_narrow: The pipeline can be called from other external 
pipelines with some default infile and outfile names, to see if this is the case and up the memory.


-20/01/2018: Affects only clustering.
-Created get_five_prime_only_single_ends: It starts with the shifted unfiltered tag aligns
and creates a 1bp region for each shifted SE with the 5' end only (strand-aware). Sorts the output

-Created filter_five_prime_only_single_ends: Gets the 5' 1bp shifted unfiltered tag aligns and filters
out any which are in areas of high or low mappability.

-Created calculateNumberOfFilteredFivePrimeOnlySingleEnds: Gets the number of 1bp 5' and filtered single end reads.

-Created/Updated overlap_sample_SE_with_all_merged_broad_narrow_peaks: For each sample, overlaps the processed, filtered, 
5' 1bp single ends with all the merged peaks for all samples to obtain 
the overlaps (unique single ends which are overlapping completely: 1bp with at least one peak). If a single end overlaps two peaks, it is only counted once.

-Created/Updated normalize_overlap_sample_with_common_peaks: For each sample (not pooled), 
divides the 5' only, filtered SE reads in each filtered peak by the total number of 5' only, filtered SE in all samples merged peaks (see the theory behind this).
To obtain the total number of 5' only, filtered SE reads in peaks: gets the total number of 5' only filtered reads (calculateNumberOfFilteredFivePrimeOnlySingleEnds) 
and multiplies by the assigned fraction taking into account all merged peaks for all samples (calculate_sample_assigned_fraction_for_all_merged_peaks).
Then divides the 5' only, filtered SE reads overlapping each common peak (merge_all_peaks) by the total number of 5' only, filtered SE reads in peaks to get a normalized measure.

-Created/Updated calculate_sample_assigned_fraction_for_all_merged_peaks: Calculates the assigned fraction for the sample filtered five prime single ends in terms of all the samples 
merged narrow and broad peaks. 

-Created/Updated overlap_sample_with_common_peaks: For each common peak to all the samples, it calculates the number of filtered 
extended 5' 1bp single ends of reads from the sample which fall in that region. An overlap is considered valid if all the SE (1bp) 
is overlapped by a feature (common peak). Produces one line for each common peak, regardless of whether there is or not intersection
    
(windows and single ends):
-Created/Updated normalize_overlap_sample_with_common_windows: For each sample, gets the total number of filtered 5' 1bp reads and divides the reads overlapping
    each constant window by the total number of filtered 5' 1bp reads of the sample to get a normalized measure.

-Created/Updated overlap_sample_with_windows: For each genomic window, it calculates the number of filtered 5' 1bp only single ends
    of reads from the sample which fall in that region. Only the 1bp overlap required.
    
(windows and peaks):
-Created/Updated sample_broad_narrow_peaks_overlap_sample_with_windows: For each genomic window, it calculates the number of merged narrow and
    broad peaks from the sample which fall in that region. Uses a fraction overlap in terms of each sample peak
    
    
(peaks in peaks):
-Created/Updated sample_broad_narrow_peaks_overlap_merged_all_samples_peaks: For each merged peak union for all samples, 
it calculates the number of merged narrow and broad peaks from the sample which fall in that region. Each sample peak 
will be completely included in all the samples peaks merged, therefore the overlaps will always be 100% and there is no need
to use a fraction overlap in terms of each sample peak.

-16/01/2018: Affects only clustering.
Changed overlap_sample_with_common_peaks, now the input is not extended single ends but filtered,
extended single ends. This is coherent with overlapping with filtered peaks generated by extended reads.

-Changed normalize_overlap_sample_with_common_peaks, which now uses as inputs: 
    -calculateNumberOfFilteredExtendedSingleEnds: Now using filtered, extended single ends and not single ends.
    -calculate_sample_assigned_fraction: Same as before using filtered, extended SE and filtered peaks.               
    -overlap_sample_with_common_peaks: with the change specified

-overlap_sample_with_windows, now the input is not extended single ends but filtered, extended single ends.

-normalize_overlap_sample_with_common_windows, now the input is not the number of single ends but number of filtered, extended single ends.


-02/01/2018: Affects only clustering.
Changed overlap of features in overlap_sample_with_common_peaks. 
A are the features where the overlap is seen (example, merged peaks from all samples)
B are the data producing overlaps (example, merged peaks for each sample)
Before the fraction of overlap required was on A (bedtools ... -f)
Now the fraction of overlap required is on B (bedtools ... -F)
Note that before, wide A features with narrow B data will produce overlaps 
which almost certainly yield no overlap despite the majority of a B data point
overlapping A.

-24/12/2017: Implemented solution for MACS2 reporting the same narrow peaks coordinates with different significance 
values: http://seqanswers.com/forums/showthread.php?t=50394
For each duplicate coordinates only the most significant peak is stored





Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_atacseq.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *
from ruffus.combinatorics import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import pipelineAtacseq
import tempfile
import re
import CGAT.IOTools as IOTools
sys.path.insert(0, "/home/mbp15ja/dev/pipeline_chromHMM/src/")
import PipelineChromHMM
sys.path.insert(0, "/home/mbp15ja/dev/AuxiliaryPrograms/Segmentations/")
import compareSegmentations as compseg
sys.path.insert(0, "/home/mbp15ja/dev/AuxiliaryPrograms/logParsers/")
import logParser
sys.path.insert(0, "/home/mbp15ja/dev/AuxiliaryPrograms/StringOperations/")
import StringOperations
sys.path.insert(0, '/home/mbp15ja/dev/AuxiliaryPrograms/Clusterings/')
import clusterings
sys.path.insert(0, '/home/mbp15ja/dev/AuxiliaryPrograms/File_operations/')
import file_operations


import matplotlib as mpl
mpl.use('Agg') # So that matplotlib.pyplot doesn't try to find the X-server and give an error
import matplotlib.pyplot as plt

import math




# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
# Specific pipeline tasks

@follows(mkdir("stats.dir"))
@transform("*.bam",
           regex("(.+).bam"),
           r"stats.dir/\1.after_mapping.tsv")
def getInitialMappingStats(infile, outfile):
    ''' Gets the initial mapping rate in terms of total reads '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    
    # The mapping file is sorted by coordinate, first sort by readname
    temp_file = P.snip(outfile, ".tsv") + ".bam"
    
    log_file = P.snip(outfile, ".tsv") + ".log"
    
    # Samtools creates temporary files with a certain prefix, create a temporal directory name
    samtools_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    samtools_temp_file = os.path.join(samtools_temp_dir, "temp")
    
    disk_space_file = P.snip(outfile, ".tsv") + ".txt"
        
    statement = ''' mkdir -p %(samtools_temp_dir)s;
      samtools sort -n -o %(temp_file)s -T %(samtools_temp_file)s %(infile)s 2> %(log_file)s;
      checkpoint;
      
    '''
    
    
    # Execute the statement
    P.run()
    
       
    
    # Get the stats
    pipelineAtacseq.getMappedUnmappedReads(temp_file, 
                       outfile, 
                       submit=True,
                       job_memory="6G")
    
    # Remove the temporal file
    statement = '''rm %(temp_file)s; 
    '''
    
    # Execute the statement
    P.run()


@follows(mkdir("first_filtering.dir"))
@transform("*.bam",
           regex("(.+).bam"),
           r"first_filtering.dir/\1.bam")
def filterOutIncorrectPairsAndExcessiveMultimappers(infile, outfile):
    
    '''Assuming a starting compressed coordinate sorted bam file. 
    Remove  unmapped, mate unmapped and reads failing platform. Keep only
    properly mapped pairs. Sort by name and filter out proper mapped pairs with
    more than the defined number of proper pair alignments'''
    
    multimappers_filter_script = PARAMS["filtering_multimappers_script"]
    
    allowed_multimappers = PARAMS["filtering_allowed_multimapper_proper_pairs"]
    
    
    log_file = P.snip(outfile, ".bam") + ".log"
    
    first_filtering_bam_output = P.snip(outfile, ".bam") + "_proper_pairs.bam"
       
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".bam") + "_temp.bam"
      
    # Samtools creates temporary files with a certain prefix
    samtools_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    


    statement = '''samtools view -F 524 -f 2 -u %(infile)s | 
    samtools sort -n - -o %(first_filtering_bam_output)s -T %(samtools_temp_file)s 2> %(log_file)s;
    checkpoint;
    
    samtools view -h %(first_filtering_bam_output)s | 
    %(multimappers_filter_script)s -k %(allowed_multimappers)s --paired-end | 
    samtools view -bS - -o %(temp_file)s 2>> %(log_file)s;
    checkpoint;
    
    mv %(temp_file)s %(outfile)s;
    checkpoint;
    
    rm %(first_filtering_bam_output)s
    '''

    P.run()





@follows(mkdir("stats.dir"))
@transform(filterOutIncorrectPairsAndExcessiveMultimappers,
           regex(".+/(.+).bam"),
           r"stats.dir/\1.after_first_filter.tsv")
def getFirstFilteringStats(infile, outfile):
    ''' Gets the mapping rate in terms of total reads after the first filtering '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    
    # The mapping file is sorted by coordinate, first sort by readname
    temp_file = P.snip(outfile, ".tsv") + ".bam"
    
    log_file = P.snip(outfile, ".tsv") + ".log"
    
    # Samtools creates temporary files with a certain prefix, create a temporal directory name
    samtools_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    samtools_temp_file = os.path.join(samtools_temp_dir, "temp")
    
    disk_space_file = P.snip(outfile, ".tsv") + ".txt"
        
    statement = ''' mkdir -p %(samtools_temp_dir)s;
      samtools sort -n -o %(temp_file)s -T %(samtools_temp_file)s %(infile)s 2> %(log_file)s;
      checkpoint;
      
    '''
    
    
    # Execute the statement
    P.run()
    
       
    
    # Get the stats
    pipelineAtacseq.getMappedUnmappedReads(temp_file, 
                       outfile, 
                       submit=True,
                       job_memory="6G")
    
    # Remove the temporal file
    statement = '''rm %(temp_file)s; 
    '''
    
    # Execute the statement
    P.run()




@follows(mkdir("second_filtering.dir"))
@transform(filterOutIncorrectPairsAndExcessiveMultimappers,
           regex(".+/(.+).bam"),
           r"second_filtering.dir/\1.bam")
def filterOutOrphanReadsAndDifferentChrPairs(infile, outfile):
    
    ''' Remove orphan reads (pair was removed) and read pairs mapping to different 
    chromosomes and read pairs which are "facing against one another" with no overlap. 
    Obtain position sorted BAM. Assumes a starting read name sorted BAM file.
    '''
    
    # Get the sample name
    sample_name , _ = os.path.splitext(os.path.basename(outfile))
    
    # Get samples details table
    sample_details = PARAMS["samples_details_table"]
    
    # Get trimmings in the 5' ends done previously (for example in qc).
    five_prime_trim = pipelineAtacseq.getSampleQCShift(sample_name, sample_details)
    
    
    integer_five_prime_correction = 0
    
    # To avoid putting "--"
    # Correction is going to be -correction on the start of the + strand
    # Correction is going to be +correction on the end of the - strand
    try:
        integer_five_prime_correction = int(five_prime_trim)
    
    except ValueError:
        
        raise Exception("Five prime trimming argument needs to be an integer.") 
    
    # String with the correction to apply (Eg. "- 2", "+ 5")
    positive_strand_correction = ""
    negative_strand_correction = ""
    
    if integer_five_prime_correction < 0:
        
        positive_strand_correction = "+ "+str(abs(integer_five_prime_correction))
        
        negative_strand_correction = "- "+str(abs(integer_five_prime_correction))
    
    elif integer_five_prime_correction > 0:
        
        positive_strand_correction = "- "+str(abs(integer_five_prime_correction))
        
        negative_strand_correction = "+ "+str(abs(integer_five_prime_correction))
    
    # 0 Case: no correction, empty string
    
    
    
    
    log_file = P.snip(outfile, ".bam") + ".log"  
    
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".bam") + "_temp.bam"
      
    # Samtools creates temporary files with a certain prefix
    samtools_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    first_filtering_bam_output = P.snip(outfile, ".bam") + "_proper_pairs.bam"
    
    # Fixmate seems to fix read pairs which are in different chromosomes but not
    # read pairs "facing away" from each other 
    # Intermediate file outputted to process these cases
    read_name_processing_input_bam = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    read_name_processing_input_bam += ".bam"
    
    read_name_processing_problematic_reads = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    read_name_processing_output_bam = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    read_name_processing_output_bam += ".bam"
    
    
    # Reads are already read name sorted, so fixmate can be run
    # Next filter the "wrong pairs" and get the read names of read pairs with each pair on a different chr
    # or "facing away" from each other with no overlap:
    # <----------                    <--------
    #           --------> Allowed             --------> Not allowed
    # Because the enzyme introduces 4 bp in the start of + strand and 5bp in the start of the - strand
    # (end coordinate in bedpe file) a minumum overlap of 10
    # The 5' ends of the reads is previously extended by any cuts performed in qc
    # which are indicated
    # Get a bed with both ends of a read pair in each line
    # and extract the problematic read names from there.
    # Then filter them out with picard FilterSamReads
    statement = '''samtools fixmate -r %(infile)s %(first_filtering_bam_output)s 2> %(log_file)s;
    checkpoint;
     
    samtools view -F 1804 -f 2 -u %(first_filtering_bam_output)s -o %(read_name_processing_input_bam)s 2>> %(log_file)s;
    checkpoint;
    
    bedtools bamtobed -bedpe -i %(read_name_processing_input_bam)s | 
    awk '($1!=$4 || ($10=="+" && $9=="-" && ($3-1%(negative_strand_correction)s-5)<($5%(positive_strand_correction)s+4))) {printf ("%%s\\n", $7)'} > %(read_name_processing_problematic_reads)s 2>> %(log_file)s;
    checkpoint;
    
    
    
    
    if [ -s %(read_name_processing_problematic_reads)s ]; 
        then 
        FilterSamReads I=%(read_name_processing_input_bam)s O=%(read_name_processing_output_bam)s READ_LIST_FILE=%(read_name_processing_problematic_reads)s      FILTER=excludeReadList 2>> %(log_file)s;
        checkpoint; 
    else 
        ln -s %(read_name_processing_input_bam)s %(read_name_processing_output_bam)s;
        checkpoint; 
    fi;
    
    
     
    samtools sort %(read_name_processing_output_bam)s -o %(temp_file)s -T %(samtools_temp_file)s 2>> %(log_file)s;
    checkpoint;
    
    mv %(temp_file)s %(outfile)s;
    checkpoint;
    
    rm %(first_filtering_bam_output)s %(read_name_processing_input_bam)s %(read_name_processing_problematic_reads)s %(read_name_processing_output_bam)s;
    '''
    
    job_memory="4G"

    P.run()




# Assumes the files are coordinate sorted
@follows(mkdir("dedupped.dir"))
@transform(filterOutOrphanReadsAndDifferentChrPairs,
           regex(".+/(.+).bam"),
           r"dedupped.dir/\1.bam")
def markDuplicates(infile, outfile):
    
    ''' Use picard to mark duplicates in BAM files (not deleted).
    The files are assumed to be coordinate sorted'''
    
    # Used to be 5G
    job_memory = "8G"
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Temporal dir, to prevent the "No space left on device" error
    temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".bam") + "_temp.bam"

    statement = ''' MarkDuplicates
                     ASSUME_SORTED=True 
                     INPUT=%(infile)s
                     OUTPUT=%(temp_file)s
                     VALIDATION_STRINGENCY=LENIENT
                     METRICS_FILE=%(outfile)s.metrics
                     REMOVE_DUPLICATES=false
                     TMP_DIR=%(temp_dir)s
                   > %(outfile)s.log;
                   checkpoint;
                   
                   mv %(temp_file)s %(outfile)s '''

    P.run()
    


@follows(mkdir("markDups_hist.dir"),
         markDuplicates)
@transform("dedupped.dir/*.bam.metrics",
           regex(".+/(.+).bam.metrics"),
           r"markDups_hist.dir/\1.png",
           r"\1")
def markDuplicatesROI(infile, outfile, sample_name):
    ''' Creates a plot from the mark duplicates metrics file showing the
    additional return of investment from further sequencing '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name + ".png"
    
    
    # Get histogram data into two numpy arrays
    sequencing_fact, roi_fact = logParser.MarkDuplicatesHistogramLogParser(infile)

    
    # Create the scatterplot
    
    # Turn interactive plotting off, we don't want to show it
    plt.ioff()
    
    # Plot the x y scatter
    plt.plot(sequencing_fact, roi_fact, '-o')
    
    plt.grid(True)
    
    
    plt.title(sample_name)
    plt.xlabel("coverage multiple")
    plt.ylabel("ROI")
    
    # Save the file, remove whitespace around the image    
    plt.savefig(outfile_temp)
    
    # Clear the plot for the next one
    plt.clf()
    
    
    # Delete the temporary files
    # Copy the temporary file to the outfile
    statement = '''touch %(outfile_temp)s;
    
                mv %(outfile_temp)s %(outfile)s;
                 
                '''
     
    P.run()
    



@follows(mkdir("stats.dir"))
@transform(markDuplicates,
           regex(".+/(.+).bam"),
           r"stats.dir/\1.after_marking_dups.tsv")
def getPostDuplicationStats(infile, outfile):
     
    ''' Assuming multimapping is allowed (multiple best alignments can occur)
    Sort the reads by readname, make filterings and get the number
    unique pair mappings:
    1) Correctly mapped pairs and primary alignments only.
    2) Correctly mapped pairs and primary or secondary alignments.
    3) Correctly mapped pairs and secondary alignments only.
    get initial statistics on the reads '''
    
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
     
    sorted_bam = P.snip(outfile, ".tsv") + "_sorted.bam"
     
    log_file = P.snip(outfile, ".tsv") + ".log"
     
    bam_outfile_sec = P.snip(outfile, ".tsv") + "_sec.bam"
     
    bam_outfile_primary = P.snip(outfile, ".tsv") + "_prim.bam"
    
    
    # Samtools creates temporary files with a certain prefix
    samtools_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
     
    # First sort the bamfile
    # Then get only the primary alignments 
    statement = '''samtools sort -n -o %(sorted_bam)s -T %(samtools_temp_file)s %(infile)s 2> %(log_file)s ;
    checkpoint;
    samtools view -h -F 256 %(sorted_bam)s -o %(bam_outfile_primary)s -U %(bam_outfile_sec)s 2>> %(log_file)s;
    '''
 
    P.run()
     
    # Now see the mapped pairs and PCR duplicates in each of the 3 files
    primary_stats_file = P.snip(outfile, ".tsv") + "_primary.tsv"
    secondary_stats_file = P.snip(outfile, ".tsv") + "_secondary.tsv"
      
    # Where only primary alignments exist (1 read = 1 alignment) 
    pipelineAtacseq.getUniquelyMappedPairsNoMultimapping(bam_outfile_primary, 
                                                  primary_stats_file, 
                                                  submit=True, 
                                                  job_memory="4G")
    
    # Where multiple alignments can exist (primary + secondary)
    pipelineAtacseq.getCorrectReadPairs(sorted_bam,
                                        outfile, 
                                        submit=True,
                                        job_memory="4G")
    
    
    # Where multiple alignments can exist (secondary)
    pipelineAtacseq.getCorrectReadPairs(bam_outfile_sec,
                                        secondary_stats_file, 
                                        submit=True,
                                        job_memory="4G")





@transform(markDuplicates,
           regex("(.+)/(.+).bam"),
           [(r"\1/\2_pos_sorted.bam"), 
            (r"\1/\2_read_name_sorted.bam")],
            r"\1/\2")
def deduplicate(infile, outfiles, sample):
    '''Remove duplicates, create final name sorted BAM. Assumes a starting position sorted BAM'''
    
    # Get both outfiles
    position_sorted_bam = outfiles[0]
    
    read_name_sorted_bam = outfiles[1]
    
    log_file = sample + ".log"
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    
    # Create end temp file and intermediate temp file for position sorted and name sorted
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file_pos_sorted_bam = P.snip(position_sorted_bam, ".bam") + "_temp.bam"
      
    # Samtools creates temporary files with a certain prefix
    samtools_pos_sorted_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    temp_file_name_sorted_bam = P.snip(read_name_sorted_bam, ".bam") + "_temp.bam"
      
    # Samtools creates temporary files with a certain prefix
    samtools_name_sorted_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    statement = '''samtools view -F 1804 -f 2 -b %(infile)s -o %(temp_file_pos_sorted_bam)s 2> %(log_file)s;
    checkpoint;
    
    samtools sort -n %(temp_file_pos_sorted_bam)s -o %(temp_file_name_sorted_bam)s -T %(samtools_name_sorted_temp_file)s 2>> %(log_file)s;
    checkpoint;
    
    mv %(temp_file_pos_sorted_bam)s %(position_sorted_bam)s;
    checkpoint;
    
    mv %(temp_file_name_sorted_bam)s %(read_name_sorted_bam)s;
    
    '''
    
    P.run()
    
    
    
@follows(mkdir("library_complexity.dir"))    
@transform(markDuplicates,
           regex(".+/(.+).bam"),
           r"library_complexity.dir/\1.pbc.qc")
def calculateLibrarycomplexity(infile, outfile):
    '''Calculates library complexity'''
      
    # outfile temp file to ensure complete execution before writing outfile
    temp_outfile = P.snip(outfile, ".pbc.qc") + "_temp.pbc.qc"
    
    # outfile temp file to ensure complete execution before writing outfile
    temp_header_outfile = P.snip(outfile, ".pbc.qc") + ".header"
      
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
      
    # Samtools creates temporary files with a certain prefix
    samtools_name_sorted_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
      
    temp_file_name_sorted_bam = P.snip(infile, ".bam") + "_temp.bam"
      
    log_file = P.snip(outfile, ".pbc.qc") + ".log"
      
      
    # 1) Turns the read name sorted file to bed with both mapped segments in the pair
    # 2) Gets the fields:
    #    -beginning of the most upstream segment
    #    -end of most downstream segment
    #    -mapping strand of each segment.
    # 3) Removes the mitochondrial chromosome regions
    # 4) Sees any repeated regions from 2), counting the times each region appears.
    # 5) Performs calculations for distinct reads, total reads and ratios.
    # 6) Creates a header file and appends the figures calculated
    header_file = "TotalReadPairs\\tDistinctReadPairs\\tOneReadPair\\tTwoReadPairs\\tNRF=Distinct/Total\\tPBC1=OnePair/Distinct\\tPBC2=OnePair/TwoPair" 
    statement = '''samtools sort -n %(infile)s -o %(temp_file_name_sorted_bam)s -T %(samtools_name_sorted_temp_file)s 2>> %(log_file)s;
    checkpoint;
      
    bedtools bamtobed -bedpe -i %(temp_file_name_sorted_bam)s | 
    awk 'BEGIN{OFS="\\t"} (($1==$4) && ($2==$5) && ($3==$6)){$9="+";$10="-"} {print $0}' | 
    awk 'BEGIN{OFS="\\t"}{print $1,$2,$4,$6,$9,$10}' | 
    grep -v 'chrM' | 
    sort | 
    uniq -c | 
    awk 'BEGIN{mt=0;m0=0;m1=0;m2=0} ($1==1){m1=m1+1} ($1==2){m2=m2+1} {m0=m0+1} {mt=mt+$1} END{printf "%%d\\t%%d\\t%%d\\t%%d\\t%%f\\t%%f\\t%%f\\n",mt,m0,m1,m2,m0/mt,m1/m0,m1/m2}' > %(temp_outfile)s;
    checkpoint;
       
    rm %(temp_file_name_sorted_bam)s %(samtools_name_sorted_temp_file)s*;
    checkpoint;
    
    echo -e '%(header_file)s' > %(temp_header_outfile)s;
    checkpoint;
    
    cat %(temp_outfile)s >> %(temp_header_outfile)s;
    checkpoint;
    
    rm %(temp_outfile)s;
    checkpoint;
      
    mv %(temp_header_outfile)s %(outfile)s;
      
    '''
  
    P.run()


   

@follows(mkdir("flagstats.dir"), deduplicate)
@transform("dedupped.dir/*_pos_sorted.bam",
           formatter(".+/(?P<SAMPLE>.+)_pos_sorted\.bam"),
           "flagstats.dir/{SAMPLE[0]}.flagstats")
def index(infile, outfile):    
    '''Index final position sorted BAM, get flag stats.'''
    
    log_file = P.snip(outfile, ".flagstats") + ".log"
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".flagstats") + "_temp.flagstats"
      
    
    # Index Final BAM file
    statement = '''samtools index %(infile)s 2> %(log_file)s;
    checkpoint;

    samtools flagstat %(infile)s > %(temp_file)s;
    checkpoint;
    
    mv %(temp_file)s %(outfile)s;
    
    '''
    
    P.run()    
    
    
    
    
@follows(mkdir("tag_align.dir"), index, deduplicate)
@transform("dedupped.dir/*_pos_sorted.bam",
           formatter(".+/(?P<SAMPLE>.+)_pos_sorted\.bam"),
           "tag_align.dir/{SAMPLE[0]}.PE2SE.tagAlign.gz")
def createTagAlign(infile, outfile):
    '''creates tagAlign file (virtual single end) with (BED 3+3 format)'''
    
    log_file = P.snip(outfile, ".PE2SE.tagAlign.gz") + ".log"
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".PE2SE.tagAlign.gz") + "_temp.PE2SE.tagAlign.gz"
    
    # Create virtual SE file containing both read pairs
    statement = '''bedtools bamtobed -i %(infile)s | 
    awk 'BEGIN{OFS="\\t"}{$4="N";$5="1000";print $0}' | 
    gzip -c > %(temp_file)s 2> %(log_file)s;
    checkpoint;
    
    mv %(temp_file)s %(outfile)s;
    '''
    
    P.run()
    


@follows(mkdir("final_tag_align.dir"), index)
@transform(createTagAlign,
           regex(".+/(.+?).PE2SE.tagAlign.gz"),
           r"final_tag_align.dir/\1.PE2SE.tagAlign.gz")    
def excludeUnwantedContigsPE2SE(infile, outfile):
    '''Exclude the contigs indicated, performs partial matching for each'''
    
    excluded_chrs = PARAMS["filtering_contigs_to_remove"]
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".PE2SE.tagAlign.gz") + "_temp.PE2SE.tagAlign.gz"
    
    statement = pipelineAtacseq.createExcludingChrFromBedStatement(infile, 
                                                                   excluded_chrs,
                                                                   temp_file)
    
    statement += ''' checkpoint;
                    mv %(temp_file)s %(outfile)s'''
    
    P.run()
    
    
@follows(mkdir("final_tag_align.dir"))
@transform(excludeUnwantedContigsPE2SE,
           regex(".+/(.+?).PE2SE.tagAlign.gz"),
           r"final_tag_align.dir/\1.PE2SE.tn5_shifted.tagAlign.gz")
def shiftTagAlign(infile, outfile):
    '''Shifts tag aligns by the TN5 sites and any 5' trimming from qc'''
    
    # Eliminate .PE2SE.tn5_shifted.tagAlign.gz from the sample name
    sample_name = re.sub('\.PE2SE\.tagAlign\.gz$', '', os.path.basename(infile))
    
    # Get samples details table
    sample_details = PARAMS["samples_details_table"]
    
    # Get trimmings in the 5' ends done previously (for example in qc).
    five_prime_trim = pipelineAtacseq.getSampleQCShift(sample_name, sample_details)
    
    integer_five_prime_correction = 0
    
    # To avoid putting "--"
    # Correction is going to be -correction on the start of the + strand
    # Correction is going to be +correction on the end of the - strand
    try:
        integer_five_prime_correction = int(five_prime_trim)
    
    except ValueError:
        
        raise Exception("Five prime trimming argument needs to be an integer.") 
    
    # String with the correction to apply (Eg. "- 2", "+ 5")
    positive_strand_correction = ""
    negative_strand_correction = ""
    
    if integer_five_prime_correction < 0:
        
        positive_strand_correction = "+ "+str(abs(integer_five_prime_correction))
        
        negative_strand_correction = "- "+str(abs(integer_five_prime_correction))
    
    elif integer_five_prime_correction > 0:
        
        positive_strand_correction = "- "+str(abs(integer_five_prime_correction))
        
        negative_strand_correction = "+ "+str(abs(integer_five_prime_correction))
    
    # 0 Case: no correction, empty string
    
    # Get the contigs
    contigs = PARAMS["general_contigs"]
    
    log_file = P.snip(outfile, ".PE2SE.tn5_shifted.tagAlign.gz") + ".tn5_shifted.log"
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".PE2SE.tn5_shifted.tagAlign.gz") + "_temp.tn5_shifted.tagAlign.gz"
    
    
    # Shift the beginning of the elements in the + strand (where the enzume cuts) by +4
    # Shift the end of the elements in the - strand (where the enzume cuts) by -5
    # Apply qc corrections too.
    statement = '''zcat %(infile)s | 
    awk -F $'\\t' 'BEGIN {OFS = FS}{ if ($6 == "+") {$2 = $2 + 4%(positive_strand_correction)s} else if ($6 == "-") {$3 = $3 - 5%(negative_strand_correction)s} print $0}' | 
    gzip -c > %(temp_file)s 2> %(log_file)s; 
    '''
        
    P.run()
    
    
    log_file_correction = P.snip(outfile, ".PE2SE.tn5_shifted.tagAlign.gz") + ".tn5_shifted_slop_correction.log"
    
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file2 = P.snip(outfile, ".PE2SE.tn5_shifted.tagAlign.gz") + "_temp_correction.tn5_shifted.tagAlign.gz"
    
    # Check that the slop does not surpass the chromosome edges and that the starts are < than ends
    PipelineChromHMM.correctSlopChromosomeEdges(temp_file, 
                                                contigs,
                                                temp_file2,
                                                log_file_correction,
                                                submit=True,
                                                job_memory="4G")
    
    # Remove the temp file
    statement = ''' rm %(temp_file)s;
    checkpoint;
    mv %(temp_file2)s %(outfile)s '''
    
    P.run()
    
    


@active_if(PARAMS["samples_pool_condition"] == 1)
@follows(mkdir("peaks_broad.dir"))
@merge([shiftTagAlign,
       "samples.tsv"],
       ["final_tag_align.dir/pooled_control.PE2SE.tn5_shifted.tagAlign.gz",
        "final_tag_align.dir/pooled_treatment.PE2SE.tn5_shifted.tagAlign.gz"])
def pool_condition_virtual_se(infiles, outfiles):
    ''' Use the design file "samples.tsv" which specifies which samples
    are control and which are condition. Then pools each sample's virtual SE 
    processed tags into the corresponding pool. '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp_pooled_control = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    outfile_temp_pooled_treatment = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # Look for the samples table
    samples_table = ""
    
    found_samples_table = False
    
    for infile in infiles:
        
        if infile == "samples.tsv":
            
            samples_table = infile
            
            found_samples_table = True
            
            break
            
    if not found_samples_table:
        
        raise Exception("The table with the samples details was not found")
    
    # Define the other variables to create the statement
    output_pooled_control = outfiles[0]
    
    output_pooled_treatment = outfiles[1]
    
    samples_directory = os.path.dirname(infiles[0])
    
    # The samples_table contains the name of the samples, but a suffix
    # has to be added to get from samples to shifted tag files
    file_suffix = ".PE2SE.tn5_shifted.tagAlign.gz"
    
    statement = pipelineAtacseq.virtualSEConditionPooling(samples_table, 
                                              outfile_temp_pooled_control, 
                                              outfile_temp_pooled_treatment,
                                              file_suffix,
                                              samples_directory)

    P.run()
    
    # Now move the files to the appropiate directory
    # Touch temp files in case they don't exist
    statement = ''' touch %(outfile_temp_pooled_control)s;
                    touch %(outfile_temp_pooled_treatment)s;
                    
                    mv %(outfile_temp_pooled_control)s %(output_pooled_control)s;
                    checkpoint;
                    
                    mv %(outfile_temp_pooled_treatment)s %(output_pooled_treatment)s;
                    '''
    
    P.run()
                    
        


@follows(mkdir("n_reads.dir"))
@transform(shiftTagAlign,
           formatter("final_tag_align.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).PE2SE.tn5_shifted.tagAlign.gz"),
           "n_reads.dir/{SAMPLE[0]}_single_end_reads.tsv")
def calculateNumberOfSingleEnds(infile, outfile):
    '''Get the number of single end reads entering the peak calling'''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    statement = '''zcat %(infile)s | wc -l > %(outfile_temp)s; 
                    checkpoint;
                    
                    touch %(outfile_temp)s;
                    mv %(outfile_temp)s %(outfile)s;
                    '''
    
    P.run()
    
    

# The pipeline can be called from other external pipelines with some default names
# Some checks are implemented here (based on the infile, outfile), to see if this is 
# the case and up the memory
@follows(mkdir("peaks_broad.dir"), 
         shiftTagAlign, 
         pool_condition_virtual_se)
@transform("final_tag_align.dir/*.PE2SE.tn5_shifted.tagAlign.gz",
           regex(".+/(.+).PE2SE.tn5_shifted.tagAlign.gz"),
           [r"peaks_broad.dir/\1_peaks.broadPeak.gz",
            r"peaks_broad.dir/\1_peaks.gappedPeak.gz",
           r"peaks_broad.dir/\1_peaks.xls"],
           r"\1")
def call_peaks_broad(infile, outfiles, sample):
    ''' Use MACS2 to calculate broad peaks and gapped peaks.
    Sorts the output by -log10pvalue,
    formats the name of the broad and gapped peaks '''
    
    # Get the thresholding values for MACS2
    threshold_method = PARAMS["macs2_threshold_method"].lower()
    
    # If nothing specified default to p (p-value)
    if threshold_method == "":
        
        threshold_method = "p"
    
    elif threshold_method not in ["p", "q"]:
        
        raise Exception("threshold method specified not valid")
    
    
    threshold_quantity = PARAMS["macs2_threshold_quantity"]
    
    # If nothing specified default to 0.01
    if threshold_quantity == "":
        
        threshold_quantity = "0.01"
        
    
    # Get the read extending and shift values
    shift_parameter = PARAMS["end_extending_shift"]
    
    # If nothing specified default to -100
    if shift_parameter == "":
        
        shift_parameter = "-100"
    
    
    
    extsize_parameter = PARAMS["end_extending_extsize"]
    
    # If nothing specified default to 200
    if extsize_parameter == "":
        
        extsize_parameter = "200"
    
    
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Get the directory for the outfile
    outdir = os.path.dirname(outfiles[0])
    
    # Create a temporal directory name for the run
    peaks_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    # Get the name of the peak file (_peaks.broadPeak.gz)
    outfile_basename = os.path.basename(outfiles[0])
    
        
    outfile_basename_prefix = P.snip(outfile_basename, "_peaks.broadPeak.gz")
    
    # Create the prefix to create the files in the temporal directory
    # and with the name of the run
    outfile_prefix = os.path.join(peaks_temp_dir, outfile_basename_prefix)
    
    
    
    # Final outfile names for the post-processed files
    broad_peaks_final_outfile = outfiles[0]
    
    gappedPeak_peaks_final_outfile = outfiles[1]
    
    
    # 1) Calculate peaks, will create .xls, .broadPeak and .gappedPeak in temp dir
    # 2) Process the .broadPeak and .gappedPeak and output in outdir
    # 3) Delete the .broadPeak and .gappedPeak temp files
    # 4) Copy the remaining created files from temp dir to outdir
    statement = '''macs2 callpeak 
    -t %(infile)s 
    -f BED 
    -n %(outfile_prefix)s 
    -g hs 
    -%(threshold_method)s %(threshold_quantity)s 
    --nomodel 
    --shift %(shift_parameter)s 
    --extsize %(extsize_parameter)s 
    --broad 
    --broad-cutoff %(threshold_quantity)s
    --keep-dup all
    --tempdir %(tmp_dir)s;
    
    checkpoint;
    
    
    sort -k 8gr,8gr %(outfile_prefix)s_peaks.broadPeak 
    | awk 'BEGIN{OFS="\\t"}{$4="Peak_"NR ; print $0}' | 
    gzip -c > %(broad_peaks_final_outfile)s;
    
    checkpoint;
    
    rm %(outfile_prefix)s_peaks.broadPeak;
    
    checkpoint;
    
    sort -k 14gr,14gr %(outfile_prefix)s_peaks.gappedPeak 
    | awk 'BEGIN{OFS="\\t"}{$4="Peak_"NR ; print $0}' | 
    gzip -c > %(gappedPeak_peaks_final_outfile)s;
    
    checkpoint;
    
    rm %(outfile_prefix)s_peaks.gappedPeak;

    checkpoint;
    
    mv %(outfile_prefix)s* %(outdir)s
    '''
    
    # The pooled datasets contain a lot of reads, it is advisable to up
    # the memory in this case
    job_memory = "4G"
    
    
    # Pooled samples for this pipeline
    if (sample == "pooled_treatment" or
        sample == "pooled_control"):
        
        job_memory = "10G"
        
    
    # Pooled samples for pipeline_atac_consensus_balanced_peaks
    elif (infile == "filtered_tag_align_pool_balanced.dir/pooled.single.end.shifted.filtered.tagAlign.gz" and
        outfiles[0] == "peaks_broad.dir/pooled_peaks.broadPeak.gz"):
        
        job_memory = "10G"    
    
    
    P.run()
    
    

# The pipeline can be called from other external pipelines with some default names
# Some checks are implemented here (based on the infile, outfile), to see if this is 
# the case and up the memory
@follows(mkdir("peaks_narrow.dir"),
         shiftTagAlign, 
         pool_condition_virtual_se)
@transform("final_tag_align.dir/*.PE2SE.tn5_shifted.tagAlign.gz",
           regex(".+/(.+).PE2SE.tn5_shifted.tagAlign.gz"),
           [r"peaks_narrow.dir/\1_peaks.narrowPeak.gz",
             r"peaks_narrow.dir/\1_control_lambda.bdg",
             r"peaks_narrow.dir/\1_peaks.xls",
             r"peaks_narrow.dir/\1_summits.bed",
             r"peaks_narrow.dir/\1_treat_pileup.bdg"],
           r"\1")
def call_peaks_narrow(infile, outfiles, sample):
    ''' Use MACS2 to calculate peaks '''
    
    # Get the thresholding values for MACS2
    threshold_method = PARAMS["macs2_threshold_method"].lower()
    
    # If nothing specified default to p (p-value)
    if threshold_method == "":
        
        threshold_method = "p"
    
    elif threshold_method not in ["p", "q"]:
        
        raise Exception("threshold method specified not valid")
    
    
    threshold_quantity = PARAMS["macs2_threshold_quantity"]
    
    # If nothing specified default to p (p-value)
    if threshold_quantity == "":
        
        threshold_quantity = "0.01"
    
    
    # Get the read extending and shift values
    shift_parameter = PARAMS["end_extending_shift"]
    
    # If nothing specified default to -100
    if shift_parameter == "":
        
        shift_parameter = "-100"
    
    
    
    extsize_parameter = PARAMS["end_extending_extsize"]
    
    # If nothing specified default to 200
    if extsize_parameter == "":
        
        extsize_parameter = "200"
    
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Get the directory for the outfile
    outdir = os.path.dirname(outfiles[0])
    
    # Create a temporal directory name for the run
    peaks_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    # Get the name of the peak file (_peaks.narrowPeak.gz)
    outfile_basename = os.path.basename(outfiles[0])
    
    outfile_basename_prefix = P.snip(outfile_basename, "_peaks.narrowPeak.gz")
    
    # Create the prefix to create the files in the temporal directory
    # and with the name of the run
    outfile_prefix = os.path.join(peaks_temp_dir, outfile_basename_prefix)
    
    
    
    # Final outfile names for the post-processed files
    narrow_peaks_final_outfile = outfiles[0]
    
    
    
    # 1) Calculate peaks, will create .xls, .narrowPeak in temp dir
    # 2) Process the .narrowPeak and output in outdir: Note that MACS2 can create multiple narrow peaks (Doesn't happen with broad peaks)
    #    with the same coordinate and different significance. According to http://seqanswers.com/forums/showthread.php?t=50394
    #    these are summits within the same peak. To sort this out, whenever there are multiple rows with the same coordinates we
    #    get the row with the highest -log10pvalue (column 8), using sort as in https://unix.stackexchange.com/questions/230040/keeping-first-instance-of-duplicates
    # 3) Delete the .narrowPeak temp files
    # 4) Copy the remaining created files from temp dir to outdir
    statement = '''macs2 callpeak 
    -t %(infile)s 
    -f BED 
    -n %(outfile_prefix)s 
    -g hs 
    -%(threshold_method)s %(threshold_quantity)s 
    --nomodel 
    --shift %(shift_parameter)s 
    --extsize %(extsize_parameter)s 
    -B 
    --SPMR 
    --keep-dup all
    --call-summits
    --tempdir %(tmp_dir)s;
    
    checkpoint;

    cat %(outfile_prefix)s_peaks.narrowPeak | sort -k1,3 -k8gr | sort -k1,3 -u | sort -k8gr  
    | awk 'BEGIN{OFS="\\t"}{$4="Peak_"NR ; print $0}' | 
    gzip -c > %(narrow_peaks_final_outfile)s;
    
    checkpoint;
    
    rm %(outfile_prefix)s_peaks.narrowPeak;
    
    checkpoint;
    
    
    mv %(outfile_prefix)s* %(outdir)s
    '''
    
    # The pooled datasets contain a lot of reads, it is advisable to up
    # the memory in this case
    job_memory = "4G"
    
    
    if (sample == "pooled_treatment" or
        sample == "pooled_control"):
        
        job_memory = "10G"
        
        
    # Pooled samples for pipeline_atac_consensus_balanced_peaks
    elif (infile == "filtered_tag_align_pool_balanced.dir/pooled.single.end.shifted.filtered.tagAlign.gz" and
        outfiles[0] == "peaks_narrow.dir/pooled_peaks.narrowPeak.gz"):
        
        job_memory = "10G"
        
    
    P.run()


@follows(call_peaks_broad,
         call_peaks_narrow)
def call_peaks():
    ''' Dummy task to sync the call of peaks '''
    pass


@follows(mkdir("filtered_peaks.dir"),
         call_peaks)
@transform(["peaks_narrow.dir/*_peaks.*.gz", "peaks_broad.dir/*_peaks.*.gz"],
           formatter(".+/(?P<SAMPLE>.+)\.(?P<EXTENSION>(narrowPeak|gappedPeak|broadPeak))\.gz"),
           "filtered_peaks.dir/{SAMPLE[0]}.{EXTENSION[0]}.gz")
def filter_peaks(infile, outfile):
    ''' Filters out regions of low mappability and excessive mappability '''
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".gz") + "_temp.gz"
    
    excluded_beds = PARAMS["filtering_bed_exclusions"]
    
    statement = pipelineAtacseq.createExcludingBedsFromBedStatement(infile, 
                                                                    excluded_beds, 
                                                                    temp_file)
    
    statement += ''' checkpoint;
                    mv %(temp_file)s %(outfile)s'''

    P.run()



@follows(mkdir("pe_bed.dir"), index, deduplicate)
@transform("dedupped.dir/*_read_name_sorted.bam",
           formatter(".+/(?P<SAMPLE>.+)_read_name_sorted\.bam"),
           "pe_bed.dir/{SAMPLE[0]}.PE.bed.gz")
def createPEBed(infile, outfile):
    '''Creates paired-end file sorted by chromosome and then by most upstream start position '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    log_file = P.snip(outfile, ".PE.bed.gz") + ".log"
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".PE.bed.gz") + "_temp.PE.bed.gz"
    
    # Create PE bed file containing both read pairs in each line
    statement = '''export TMPDIR=%(tmp_dir)s;
                bedtools bamtobed -bedpe -i  %(infile)s |  
                sort -k1,1 -k2,2n |  
                gzip -c > %(temp_file)s;
                
                checkpoint; 

                mv %(temp_file)s %(outfile)s;
                '''
    
    P.run()


    
@follows(mkdir("final_pe_bed.dir"), index)
@transform(createPEBed,
           regex(".+/(.+?).PE.bed.gz"),
           r"final_pe_bed.dir/\1.PE.bed.gz")    
def excludeUnwantedContigsPEBed(infile, outfile):
    '''Exclude the contigs indicated, performs partial matching for each'''
    
    excluded_chrs = PARAMS["filtering_contigs_to_remove"]
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".PE.bed.gz") + "_temp.PE.bed.gz"
    
    statement = pipelineAtacseq.createExcludingChrFromBedStatement(infile, 
                                                                   excluded_chrs,
                                                                   temp_file)
    
    statement += ''' checkpoint;
                    mv %(temp_file)s %(outfile)s'''
    
    P.run()  
    


@follows(mkdir("final_pe_bed.dir"))
@transform(excludeUnwantedContigsPEBed,
           regex(".+/(.+?).PE.bed.gz"),
           r"final_pe_bed.dir/\1.PE.tn5_shifted.bed.gz")
def shiftPEBed(infile, outfile):
    '''Shifts tag aligns by the TN5 sites and any 5' trimming from qc'''
    
    # Eliminate .PE2SE.tn5_shifted.tagAlign.gz from the sample name
    sample_name = re.sub('\.PE\.bed\.gz$', '', os.path.basename(infile))
    
    # Get samples details table
    sample_details = PARAMS["samples_details_table"]
    
    # Get trimmings in the 5' ends done previously (for example in qc).
    five_prime_trim = pipelineAtacseq.getSampleQCShift(sample_name, sample_details)
    
    integer_five_prime_correction = 0
    
    # To avoid putting "--"
    # Correction is going to be -correction on the start of the + strand
    # Correction is going to be +correction on the end of the - strand
    try:
        integer_five_prime_correction = int(five_prime_trim)
    
    except ValueError:
        
        raise Exception("Five prime trimming argument needs to be an integer.") 
    
    # String with the correction to apply (Eg. "- 2", "+ 5")
    positive_strand_correction = ""
    negative_strand_correction = ""
    
    if integer_five_prime_correction < 0:
        
        positive_strand_correction = "+ "+str(abs(integer_five_prime_correction))
        
        negative_strand_correction = "- "+str(abs(integer_five_prime_correction))
    
    elif integer_five_prime_correction > 0:
        
        positive_strand_correction = "- "+str(abs(integer_five_prime_correction))
        
        negative_strand_correction = "+ "+str(abs(integer_five_prime_correction))
    
    # 0 Case: no correction, empty string
    
    # Get the contigs
    contigs = PARAMS["general_contigs"]
    
    log_file = P.snip(outfile, ".PE.tn5_shifted.bed.gz") + ".PE.tn5_shifted.log"
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".PE.tn5_shifted.bed.gz") + "_temp.tn5_shifted.bed.gz"
    
    
    # Shift the beginning of the elements in the + strand (where the enzume cuts) by +4
    # Shift the end of the elements in the - strand (where the enzume cuts) by -5
    # Apply qc corrections too.
    # We have a bed pe file, we apply corrections for the + and - strand simultaneously
    # (in each line). We take into account the order in which the + and -
    # segments come to do this. $9 == "+" -> + - case. $9 == "-" -> - + case
    statement = '''zcat %(infile)s | 
    awk -F $'\\t' 'BEGIN {OFS = FS}{ if ($9 == "+") {$2 = $2 + 4%(positive_strand_correction)s; $6 = $6 - 5%(negative_strand_correction)s} else if ($9 == "-") {$5 = $5 + 4%(positive_strand_correction)s; $3 = $3 - 5%(negative_strand_correction)s} print $0}' | 
    gzip -c > %(temp_file)s 2> %(log_file)s; 
    '''
        
    P.run()
    
    
    log_file_correction = P.snip(outfile, ".PE.tn5_shifted.bed.gz") + ".tn5_shifted_slop_correction.log"
    
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file2 = P.snip(outfile, ".PE.tn5_shifted.bed.gz") + "_temp_correction.tn5_shifted.bed.gz"
    
    # Check that the slop does not surpass the chromosome edges and that the starts are < than ends
    PipelineChromHMM.correctSlopChromosomeEdgesBedPE(temp_file, 
                                                contigs,
                                                temp_file2,
                                                log_file_correction,
                                                submit=True,
                                                job_memory="4G")
    
    # Remove the temp file
    statement = ''' rm %(temp_file)s;
    checkpoint;
    mv %(temp_file2)s %(outfile)s '''
    
    P.run()
    





@follows(mkdir("histogram_five_prime_dist.dir"))
@transform(shiftPEBed,
           regex(".+/(.+).PE.tn5_shifted.bed.gz"),
           r"histogram_five_prime_dist.dir/\1_cut_dist_histogram.png")
def generateHistogramFivePrimeInsertSize(infile, outfile):
    '''Generates a histogram of the distances between 5' cuts by the TN5 enzyme'''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_values_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name + ".png"
    
    
    
    statement = ''''''
    
    # Depending on if the + strand read is first or the - strand read is first, 
    # get the 5' ends of the reads (start in the + strand and end in
    # the - strand).
    # Store chr_upstream_read, chr_downstream_read, five_prime_plus, five_prime_minus     
    statement += '''zcat %(infile)s | 
                awk -F $'\\t' '{five_prime_plus=0;five_prime_minus=0} ($9=="+"){five_prime_plus=$2;five_prime_minus=$6} ($9=="-"){five_prime_plus=$5;five_prime_minus=$3} {printf "%%s\\t%%s\\t%%s\\t%%s\\n",$1,$4,five_prime_plus,five_prime_minus}' | ''' 
    
    # Only store the read pairs aligning to the same chromosome
    statement += '''awk -F $'\\t' '($1==$2) {printf ("%%s\\n", $0)'} | ''' 
    
    # Calculate the difference between the five primes, taking into account that the - strand 
    # five prime end coordinate is not included (bed).
    # Since field $3 contains the five_prime of the + strand and
    # field $4 contains the five_prime of the - strand. Only when
    # $4 >= ($3+1) we return a difference.
    # $4 < ($3+1) means:
    #      |------->
    # <--|   
    # Doesn't make sense                                                            
    statement += '''awk -F $'\\t' '{difference=($4-($3+1))} (difference>=0){printf ("%%s\\n", difference)'} > %(outfile_values_temp)s;
    '''
        
    P.run()
     
    
    # Create the histogram 
    pipelineAtacseq.histogramFromOneDValues(outfile_values_temp,
                                            30,
                                            outfile_temp,
                                            submit=True, 
                                            job_memory="4G")
    
    
    # Delete the temporary files
    # Copy the temporary file to the outfile
    statement = '''touch %(outfile_temp)s;
    
                rm %(outfile_values_temp)s;
                
                mv %(outfile_temp)s %(outfile)s;
                 
                '''
     
    P.run()
    
    


@follows(mkdir("windows_genome.dir"))
@originate("windows_genome.dir/window."+str(PARAMS["naive_feature_clustering_window_width"])+".bed.gz") 
def make_genomic_windows(outfile):
    ''' Makes genomic windows of width specified '''
    
    window_width = PARAMS["naive_feature_clustering_window_width"]
    
    # Get the contigs
    contigs = PARAMS["general_contigs"]
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    statement = '''bedtools makewindows -g %(contigs)s -w %(window_width)s |  
    sort -k 1,1 -k2,2n |  
    gzip -c > %(outfile_temp)s;
    checkpoint;
    
    touch %(outfile_temp)s;
    mv %(outfile_temp)s %(outfile)s;
    
    '''
    
    P.run()






@follows(mkdir("common_peaks.dir"))
@collate(filter_peaks,
        regex(r".+\.dir/((?!pooled_[control|treatment]).+?)_peaks\.(narrowPeak|broadPeak)\.gz$"),
        "common_peaks.dir/all_merged_narrow_broad_peaks.gz")
def merge_all_peaks(infiles, outfile):
    ''' Merges all the narrow peaks and broad peaks produced by all
    the samples which are within a specified merging range.
    Outputs a coordinate sorted file'''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # For now 100, should be defined in pipeline.ini
    merging_range = "100"
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    statement = pipelineAtacseq.merge_all_narrow_peaks_statement(infiles, outfile_temp, merging_range)
    
    P.run()

    # Now move the files to the appropiate directory
    # Touch temp files in case they don't exist
    statement = ''' touch %(outfile_temp)s;
                                        
                    mv %(outfile_temp)s %(outfile)s;
                    
                    '''
    
    P.run()
    




# Analogous to merge_all_peaks
@follows(mkdir("sample_narrow_broad_peaks.dir"))
@collate(filter_peaks,
        regex(r".+\.dir/(?!pooled_[control|treatment])(.+?)_peaks\.(narrowPeak|broadPeak)\.gz$"),
        r"sample_narrow_broad_peaks.dir/\1_merged_narrow_broad_peaks.gz")
def merge_all_peaks_per_sample(infiles, outfile):
    ''' Merges all the narrow peaks and broad peaks produced per 
    sample which are within a specified merging range.
    Outputs a coordinate sorted file. '''
    
    # Reuse the method with the specified files per sample
    merge_all_peaks(infiles, outfile)
    
    
    
    
# Analogous to merge_all_peaks
@follows(mkdir("pooled_narrow_broad_peaks.dir"))
@collate(filter_peaks,
        regex(r".+\.dir/((pooled_[control|treatment]).+?)_peaks\.(narrowPeak|broadPeak)\.gz$"),
        r"pooled_narrow_broad_peaks.dir/\1_merged_narrow_broad_peaks.gz")
def merge_all_peaks_per_pooled_sample(infiles, outfile):
    ''' Merges all the narrow peaks and broad peaks produced for the pooled 
    samples which are within a specified merging range.
    Outputs a coordinate sorted file. '''
    
    # Reuse the method with the specified files per sample
    merge_all_peaks(infiles, outfile)





   


@follows(mkdir("IGV.dir"))
@transform(shiftTagAlign,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).PE2SE.tn5_shifted.tagAlign.gz"),
           "IGV.dir/{SAMPLE[0]}.single.end.processed.extended.tagAlign.gz")
def extend_five_prime_single_ends(infile, outfile):
    ''' It transforms the 5' ends of the shifted reads to make them
    an extension by the values defined in call_peaks,
    reflecting the input file to the peak caller. Sorts the output '''
      
    # Get the read extending and shift values
    shift_parameter = PARAMS["end_extending_shift"]
    
    # Negative of the shift parameter
    negative_shift_parameter = ""
    
    # If nothing specified default to -100
    if shift_parameter == "":
        
        shift_parameter = "-100"
        
        # Include the + sign to include the string as arithmetic operation
        negative_shift_parameter = "+100"
        
    # Check the shift parameter is not positive
    integer_shift_parameter = 0
    
    try:
        integer_shift_parameter = int(shift_parameter)
    
    # If its not an integer
    except ValueError:
        
        raise Exception("Non integer specified for shift parameter")
    
    # If its positive
    if integer_shift_parameter > 0:
        
        raise Exception("Positive integer specified for shift parameter")
    
    # If the parameter is "0" leave the empty string so it won't do
    # any arithmetic on the regions
    if shift_parameter == "0":
        
        shift_parameter = ""
        
        negative_shift_parameter = ""
        
    # If the shift parameter is negative and integer non-zero, get the negative
    else:
    
        # Include the + sign to include the string as arithmetic operation
        negative_shift_parameter = "+"+str(integer_shift_parameter*(-1))
    
        # Report shift_parameter as a string
        shift_parameter = str(integer_shift_parameter)
    
    extsize_parameter = PARAMS["end_extending_extsize"]
    
    # Negative of the extsize parameter
    negative_extsize_parameter = ""
    
    # If nothing specified default to 200
    if extsize_parameter == "":
        
        # Include the + sign to include the string as arithmetic operation
        extsize_parameter = "+200"
        
        negative_extsize_parameter = "-200"
        
    # Check the extsize parameter is not negative
    integer_extsize_parameter = 0
    
    try:
        integer_extsize_parameter = int(extsize_parameter)
    
    # If its not an integer
    except ValueError:
        
        raise Exception("Non integer specified for extsize parameter")
    
    # If its negative
    if integer_extsize_parameter < 0:
        
        raise Exception("Negative integer specified for extsize parameter")
    
    # If the parameter is "0" leave the empty string so it won't do
    # any arithmetic on the regions
    if extsize_parameter == "0":
        
        extsize_parameter = ""
        
        negative_extsize_parameter = ""
        
    # If the extsize parameter is negative and integer non-zero, get the negative
    else:
    
        negative_extsize_parameter = str(integer_extsize_parameter*(-1))
    
        # Include the + sign to include the string as arithmetic operation
        extsize_parameter = "+"+str(integer_extsize_parameter)
    
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # To make sure the run is complete before copying the result to the outfile
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    
    # Extends and sorts the tagAlignFile
    statement = '''zcat %(infile)s | awk -F $'\\t' 'BEGIN {OFS = FS}{ if ($6 == "+") {$2 = $2 %(shift_parameter)s; $3 = $2 + 1 %(extsize_parameter)s} else if ($6 == "-") {$3 = $3 %(negative_shift_parameter)s; $2 = $3 - 1 %(negative_extsize_parameter)s} print $0}' | sort -k 1,1 -k2,2n | gzip -c > %(outfile_temp)s;
    checkpoint;
    
    mv %(outfile_temp)s %(outfile)s
    '''
    
    P.run()
    



@follows(mkdir("assigned_fraction.dir"))
@transform(extend_five_prime_single_ends,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).single.end.processed.extended.tagAlign.gz"),
           "assigned_fraction.dir/{SAMPLE[0]}.single.end.processed.extended.filtered.tagAlign.gz")
def filter_extended_five_prime_single_ends(infile, outfile):
    ''' Filters out regions of low mappability and excessive mappability in the extended single ends'''
    
    # Reuse the filter_peaks function
    filter_peaks(infile, outfile)
    
    



@follows(mkdir("IGV.dir"))
@transform(shiftTagAlign,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).PE2SE.tn5_shifted.tagAlign.gz"),
           "IGV.dir/{SAMPLE[0]}.five.prime.only.single.end.processed.tagAlign.gz")
def get_five_prime_only_single_ends(infile, outfile):
    ''' It creates a 1bp region for each shifted SE with the 5' end only (strand-aware).
    Sorts the output '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # To make sure the run is complete before copying the result to the outfile
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    
    # Gets a 1bp region with the 5' end (strand-aware) and sorts the tagAlignFile
    statement = '''zcat %(infile)s | awk 'BEGIN {FS ="\\t"; OFS = FS} { if ($6 == "+") {$3 = $2+1} else if ($6 == "-") {$2 = $3-1} print $0}' | sort -k 1,1 -k2,2n | gzip -c > %(outfile_temp)s;
    checkpoint;
    
    mv %(outfile_temp)s %(outfile)s
    '''
    
    P.run()
    



# Analogous to filter_extended_five_prime_single_ends    
@follows(mkdir("assigned_fraction.dir"))
@transform(get_five_prime_only_single_ends,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).five.prime.only.single.end.processed.tagAlign.gz"),
           "assigned_fraction.dir/{SAMPLE[0]}.five.prime.only.single.end.processed.filtered.tagAlign.gz")
def filter_five_prime_only_single_ends(infile, outfile):
    ''' Filters out regions of low mappability and excessive mappability in the 1bp five prime single ends'''
    
    # Reuse the filter_peaks function
    filter_peaks(infile, outfile)




# Reuse calculateNumberOfSingleEnds   
@follows(mkdir("n_reads.dir"))
@transform(filter_five_prime_only_single_ends,
           formatter("assigned_fraction.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).five.prime.only.single.end.processed.filtered.tagAlign.gz"),
           "n_reads.dir/{SAMPLE[0]}_single_end_five_prime_only_filtered_reads.tsv")
def calculateNumberOfFilteredFivePrimeOnlySingleEnds(infile, outfile):
    '''Get the number of 1bp five prime and filtered single end reads'''
    
    calculateNumberOfSingleEnds(infile, outfile)




@follows(merge_all_peaks_per_sample)
@transform(filter_extended_five_prime_single_ends,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).single.end.processed.extended.filtered.tagAlign.gz"),
           add_inputs("sample_narrow_broad_peaks.dir/{SAMPLE[0]}_merged_narrow_broad_peaks.gz"),
           "assigned_fraction.dir/{SAMPLE[0]}.SE.processed.extended.filtered_intersect_merged_narrow_broad_peaks.gz")
def overlap_sample_SE_with_merged_broad_narrow_peaks(infiles, outfile):
    ''' For each sample, overlaps the processed, filtered, extended single ends with
     the peaks to obtain the overlaps (single ends which are overlapping at least one peak).
     If a single end overlaps two peaks, it is only counted once. 
     An overlap of 1bp is sufficient.'''
    
    # Separate the peaks and SE files
    peaks = infiles[1]
    
    single_ends = infiles[0]
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Temp files for sorting
    SE_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    peaks_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
     
    # To make sure the run is complete before copying the result to the outfile
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # Sort the SE and peaks files
    statement = '''zcat %(single_ends)s | sort -k1,1 -k2,2n | gzip -c > %(SE_temp)s; 
                checkpoint; 
                
                zcat %(peaks)s | sort -k1,1 -k2,2n | gzip -c > %(peaks_temp)s; 
                checkpoint; 
                '''
         
    # Intersect the single ends with peaks returning only each single end once which is
    # overlapped by a peak
    statement += '''bedtools intersect -u -sorted -a %(SE_temp)s -b %(peaks_temp)s | 
                    sort -k1,1 -k2,2n | 
                    awk -F '\\t' '{OFS="\\t"} {printf "%%s\\t%%s\\t%%s\\n", $1,$2,$3}' | 
                    gzip -c > %(outfile_temp)s;
                    checkpoint; 
                     
    '''
 
    # Don't execute now but add to later
    statement += '''mv %(outfile_temp)s %(outfile)s; 
    '''
    
    job_memory = "4G"
     
    P.run()
    
    
    
    
    
    
# Analogous to overlap_sample_SE_with_merged_broad_narrow_peaks but using all the merged peaks (for all samples).
# No fraction overlap required. If this changes, an independent function should be used.
@follows(merge_all_peaks)
@transform(filter_five_prime_only_single_ends,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).five.prime.only.single.end.processed.filtered.tagAlign.gz"),
           add_inputs(merge_all_peaks),
           "assigned_fraction.dir/{SAMPLE[0]}.SE.five.prime.only.filtered_intersect_all_merged_narrow_broad_peaks.gz")
def overlap_sample_SE_with_all_merged_broad_narrow_peaks(infiles, outfile):
    ''' For each sample, overlaps the processed, filtered, 5' 1bp single ends with
     all the merged peaks for all samples to obtain the overlaps (single ends which are overlapping completely: 1bp with 
     at least one peak).
     If a single end overlaps two peaks, it is only counted once.'''
    
    # Reuse the function
    overlap_sample_SE_with_merged_broad_narrow_peaks(infiles, outfile)
    
   
    
    
@follows(overlap_sample_SE_with_merged_broad_narrow_peaks)   
@transform(filter_extended_five_prime_single_ends,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).single.end.processed.extended.filtered.tagAlign.gz"),
           add_inputs("assigned_fraction.dir/{SAMPLE[0]}.SE.processed.extended.filtered_intersect_merged_narrow_broad_peaks.gz"),
           "assigned_fraction.dir/{SAMPLE[0]}.assigned_fraction.tsv")               
def calculate_sample_assigned_fraction(infiles, outfile):
    ''' Calculates the assigned fraction for each sample: 
    Number of proper processed, non mitochondrial filtered SE reads overlapping peaks/Total of proper processed, non mitochondrial filtered SE reads'''
    
    # Separate the infiles
    total_single_ends = infiles[0]
    
    single_ends_overlapping_peaks = infiles[1]
    
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Temp files to count the number of single ends
    SE_total = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    SE_overlapping_peaks = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
     
    # To make sure the run is complete before copying the result to the outfile
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    # Reuse the function to calculate the number of single ends (lines from a file)
    calculateNumberOfSingleEnds(total_single_ends, SE_total)
    
    
    calculateNumberOfSingleEnds(single_ends_overlapping_peaks, SE_overlapping_peaks)
    
    
    # Parse the files with the results
    number_total_single_ends = logParser.readSingleNumberFile(SE_total)
    
    number_single_ends_overlapping_peaks = logParser.readSingleNumberFile(SE_overlapping_peaks)
    
    
    # Get the fraction as a percentage
    assigned_fraction = str(
        (float(number_single_ends_overlapping_peaks)/float(number_total_single_ends))
        *100.0)
    
    # Output it
    with open(outfile, "w") as writer:
        
        writer.write(assigned_fraction)
        
    writer.close()
    
    


# Analogous to calculate_sample_assigned_fraction. Calculates assigned fraction for each sample extended and filtered SE
# taking into account all merged peaks for all samples and fraction overlap of the SE.
@follows(overlap_sample_SE_with_all_merged_broad_narrow_peaks)   
@transform(filter_five_prime_only_single_ends,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).five.prime.only.single.end.processed.filtered.tagAlign.gz"),
           add_inputs("assigned_fraction.dir/{SAMPLE[0]}.SE.five.prime.only.filtered_intersect_all_merged_narrow_broad_peaks.gz"),
           "assigned_fraction.dir/{SAMPLE[0]}.assigned_fraction_all_merged_narrow_broad_peaks.tsv") 
def calculate_sample_assigned_fraction_for_all_merged_peaks(infiles, outfile):
    '''Calculates the assigned fraction for the sample filtered five prime single ends in terms of all the samples merged narrow and broad peaks.'''
    
    # Reuse the function
    calculate_sample_assigned_fraction(infiles, outfile)


    
    

@follows(mkdir("common_peaks.dir"))
@transform(filter_five_prime_only_single_ends,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).five.prime.only.single.end.processed.filtered.tagAlign.gz"),
           add_inputs(merge_all_peaks),
           "common_peaks.dir/{SAMPLE[0]}_overlap_common_narrow_broad_peaks.gz")
def overlap_sample_with_common_peaks(infiles, outfile):
    ''' For each common peak to all the samples, it calculates the number of filtered extended 5' 1bp single ends
    of reads from the sample which fall in that region. An overlap is considered valid if all the SE (1bp) is overlapped 
    by a feature (common peak). 
    Produces one line for each common peak, regardless of whether there is or not intersection'''
    extended_tag_align = infiles[0]
    merged_common_peaks = infiles[1]
     
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # To make sure the run is complete before copying the result to the outfile
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
     
    # The common_peaks file is already sorted
    # Intersect and count the number of reads in each common_peak
    statement = '''bedtools intersect -sorted -c -a %(merged_common_peaks)s -b %(extended_tag_align)s | gzip -c > %(outfile_temp)s;
    checkpoint; 
    '''
 
    # Don't execute now but add to later
    statement += '''mv %(outfile_temp)s %(outfile)s; 
    '''
     
    P.run()


# Analogous to overlap_sample_with_common_peaks
@follows(mkdir("common_windows.dir"))
@transform(filter_five_prime_only_single_ends,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+).five.prime.only.single.end.processed.filtered.tagAlign.gz"),
           add_inputs(make_genomic_windows),
           "common_windows.dir/{SAMPLE[0]}_overlap_common_windows.gz")
def overlap_sample_with_windows(infiles, outfile):
    ''' For each genomic window, it calculates the number of filtered 5' 1bp only single ends
    of reads from the sample which fall in that region. Only the 1bp overlap required. '''
    
    # Reuse the function inputting constant genomic windows instead of peaks since it doesn't use a fraction of overlap
    overlap_sample_with_common_peaks(infiles, outfile)
    
    
    

# Analogous to overlap_sample_with_common_peaks but uses a fraction overlap in terms of each sample peak
# Therefore requires it's own method
@transform(merge_all_peaks_per_sample,
           regex(r".+\.dir/(?!pooled_[control|treatment])(.+)_merged_narrow_broad_peaks.gz"),
           add_inputs(make_genomic_windows),
           r"sample_narrow_broad_peaks.dir/\1_merged_narrow_broad_peaks_overlap_common_windows.gz")
def sample_broad_narrow_peaks_overlap_sample_with_windows(infiles, outfile):
    ''' For each genomic window, it calculates the number of merged narrow and
    broad peaks from the sample which fall in that region. Uses a fraction overlap in terms of each sample peak '''
    
    peaks_per_sample = infiles[0]
    common_windows = infiles[1]
     
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # To make sure the run is complete before copying the result to the outfile
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # Get the fraction overlap
    fraction_overlap = PARAMS["clustering_feature_overlap_fraction"]
     
    # The common_windows and peaks_per_sample file is already sorted
    # Intersect and count the number of peaks in each common_window
    # For the intersection it requires that at least a specified fraction
    # of each feature is overlapped.
    statement = '''bedtools intersect -sorted -c -F %(fraction_overlap)s -a %(common_windows)s -b %(peaks_per_sample)s | gzip -c > %(outfile_temp)s;
    checkpoint; 
    '''
  
    # Don't execute now but add to later
    statement += '''mv %(outfile_temp)s %(outfile)s; 
    '''
      
    P.run()

    
    

   
@follows(mkdir("common_peaks.dir"),
         calculateNumberOfFilteredFivePrimeOnlySingleEnds,
         calculate_sample_assigned_fraction_for_all_merged_peaks)               
@transform(overlap_sample_with_common_peaks,
           formatter("common_peaks.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+)_overlap_common_narrow_broad_peaks.gz"),
           add_inputs(["n_reads.dir/{SAMPLE[0]}_single_end_five_prime_only_filtered_reads.tsv",
                       "assigned_fraction.dir/{SAMPLE[0]}.assigned_fraction_all_merged_narrow_broad_peaks.tsv"]),
           "common_peaks.dir/{SAMPLE[0]}_normalized_overlap_common_narrow_broad_peaks.gz")
def normalize_overlap_sample_with_common_peaks(infiles, outfile):
    
    ''' For each sample (not pooled), divides the reads in each filtered peak by the total number of 5' only, filtered SE in all samples merged peaks 
    (see the theory behind this).
    To obtain the total number of reads in peaks: gets the total number of 5' only filtered reads (calculateNumberOfFilteredFivePrimeOnlySingleEnds) 
    and multiplies by the assigned fraction taking into account all merged peaks for all samples (calculate_sample_assigned_fraction_for_all_merged_peaks).
    Then divides the reads overlapping each common peak (merge_all_peaks) by the total number of reads in peaks to get a normalized measure.
    '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    overlap_sample_with_common_peaks_file = infiles[0]
    
    number_of_single_ends_file = infiles[1][0]
    
    assigned_fraction_file = infiles[1][1]
    
    sample_number_single_ends = int(logParser.readSingleNumberFile(number_of_single_ends_file))
    
    # The assigned fraction comes as a percentage (not fraction)
    sample_assigned_fraction = float(logParser.readSingleFloatFile(assigned_fraction_file))
    
    # Calculate the number of reads in peaks
    sample_reads_in_peaks = int(math.ceil(float(sample_number_single_ends)*(sample_assigned_fraction/100.0)))
    
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # Get the number of reads in peaks in the sample
    statement = '''number_of_reads_in_peaks=%(sample_reads_in_peaks)s;
                checkpoint; '''
    
    
    # Get the log(10), tells us how many figures needed to represent 
    # 1/number_of_reads_in_peaks (the number to one significant figure)
    statement += '''log_numb_reads=$(echo 'l('$number_of_reads_in_peaks')/l(10)' | bc -l);
                checkpoint; '''
    
    # Get the ceiling for that number (returns a integer as a float)
    statement += '''ceiling_decimal_places=$(python -c "from math import ceil; print ceil($log_numb_reads)");
                checkpoint; '''
    
    # Get the integer part
    statement += '''decimal_places=${ceiling_decimal_places%%.*};
                checkpoint; '''

    # Increase the precision by 5 more figures                
    statement += '''((decimal_places=$decimal_places+5));
                checkpoint; '''
    
    # Output to those the normalized count to those many decimals
    statement += '''zcat %(overlap_sample_with_common_peaks_file)s | 
                awk '{printf ("%%s\\t%%s\\t%%s\\t%%.'$decimal_places'f\\n", $1, $2, $3, $4/'$number_of_reads_in_peaks')'} | gzip -c > %(outfile_temp)s;
                checkpoint; 
                
                touch %(outfile_temp)s;
                
                mv %(outfile_temp)s %(outfile)s
                '''
    
    P.run()
    
    

# Normalization in terms of total reads since we are putting all reads in windows
@follows(mkdir("common_windows.dir"),
         calculateNumberOfFilteredFivePrimeOnlySingleEnds)
@transform(overlap_sample_with_windows,
           formatter("common_windows.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+)_overlap_common_windows.gz"),
           add_inputs("n_reads.dir/{SAMPLE[0]}_single_end_five_prime_only_filtered_reads.tsv"),
           "common_windows.dir/{SAMPLE[0]}_normalized_overlap_common_windows.gz")
def normalize_overlap_sample_with_common_windows(infiles, outfile):
    
    ''' For each sample, gets the total number of filtered 5' 1bp reads and divides the reads overlapping
    each constant window by the total number of filtered 5' 1bp reads of the sample to get a normalized measure'''
      
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    overlap_sample_with_common_peaks_file = infiles[0]
    
    number_of_single_ends_file = infiles[1]
    
    sample_number_single_ends = logParser.readSingleNumberFile(number_of_single_ends_file)
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # Get the number of reads in the sample
    statement = '''number_of_reads=%(sample_number_single_ends)s;
                checkpoint; '''
    
    
    # Get the log(10), tells us how many figures needed to represent 
    # 1/number_of_reads (the number to one significant figure)
    statement += '''log_numb_reads=$(echo 'l('$number_of_reads')/l(10)' | bc -l);
                checkpoint; '''
    
    # Get the ceiling for that number (returns a integer as a float)
    statement += '''ceiling_decimal_places=$(python -c "from math import ceil; print ceil($log_numb_reads)");
                checkpoint; '''
    
    # Get the integer part
    statement += '''decimal_places=${ceiling_decimal_places%%.*};
                checkpoint; '''

    # Increase the precision by 5 more figures                
    statement += '''((decimal_places=$decimal_places+5));
                checkpoint; '''
    
    # Output to those the normalized count to those many decimals
    statement += '''zcat %(overlap_sample_with_common_peaks_file)s | 
                awk '{printf ("%%s\\t%%s\\t%%s\\t%%.'$decimal_places'f\\n", $1, $2, $3, $4/'$number_of_reads')'} | gzip -c > %(outfile_temp)s;
                checkpoint; 
                
                touch %(outfile_temp)s;
                
                mv %(outfile_temp)s %(outfile)s
                '''
    
    P.run()
    
   
    
    
    

@transform(sample_broad_narrow_peaks_overlap_sample_with_windows,
           regex(r"sample_narrow_broad_peaks.dir/(?!pooled_[control|treatment])(.+)_merged_narrow_broad_peaks_overlap_common_windows.gz"),
           r"sample_narrow_broad_peaks.dir/\1_binarized_merged_narrow_broad_peaks_overlap_common_windows.gz")
def binarize_sample_broad_narrow_peaks_overlap_sample_with_common_windows(infile, outfile):
    
    ''' For each sample, it binarizes the number of merged narrow and broad peaks
    overlapping each constant window '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    statement = '''zcat %(infile)s | awk -F '\\t' '{OFS="\\t";binary_result=0} ($4>=1){binary_result=1} {printf "%%s\\t%%s\\t%%s\\t%%s\\n",$1,$2,$3,binary_result}' | gzip -c > %(outfile_temp)s; 
    checkpoint;
    
    touch %(outfile_temp)s;
                
    mv %(outfile_temp)s %(outfile)s
    '''
    
    P.run()
    
    
    
    
    
    
    
@follows(mkdir("n_overlaps_common_peaks.dir"))
@merge(normalize_overlap_sample_with_common_peaks,
       "n_overlaps_common_peaks.dir/normalized_overlap_common_narrow_broad_peaks_matrix.gz")
def group_normalized_overlap_sample_with_common_peaks(infiles, outfile):
    ''' Groups the normalized overlaps of all samples with the common peaks into one matrix '''
    
    # Create the statement to be filled
    statement = ""
    
    # Space separated string of the file names to maintain the order of adding of files
    bed_files = ""
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # To keep track of the position of each cell line on the table
    position_counter = 0
    
    for infile in infiles:
        
        # Store the cell_line name in an array
        statement += '''array_cell_lines['''+str(position_counter)+''']="'''+infile+'''"; '''
        
        # Keep track of the files being added
        # If it is the first file added, no space
        if position_counter == 0:
            
            
            bed_files += infile
            
        else:
            
            bed_files += " " +infile
            
        
        
        position_counter += 1
    
    # If there are at least two files    
    if (position_counter > 1):
        # Combine all the files taking into account the common regions
        statement += '''bedtools unionbedg -header -names "${array_cell_lines[@]}" -i %(bed_files)s | gzip -c > %(outfile_temp)s;
                        checkpoint;
                        
                        touch %(outfile_temp)s;
                    
                        mv %(outfile_temp)s %(outfile)s
                    '''
            
        P.run()
        




# Analogous to group_normalized_overlap_sample_with_common_peaks
@follows(mkdir("n_overlaps_common_windows.dir"))
@merge(normalize_overlap_sample_with_common_windows,
       "n_overlaps_common_windows.dir/normalized_overlap_common_windows_matrix.gz")
def group_normalized_overlap_sample_with_windows(infiles, outfile):
    ''' Groups the normalized overlaps of all samples with the constant windows into one matrix '''
    
    # Reuse the function inputting constant genomic windows instead of peaks
    group_normalized_overlap_sample_with_common_peaks(infiles, outfile)

        






# Analogous to group_normalized_overlap_sample_with_common_peaks
@follows(mkdir("n_broad_narrow_peaks_overlaps_common_windows.dir"))
@merge(binarize_sample_broad_narrow_peaks_overlap_sample_with_common_windows,
       "n_broad_narrow_peaks_overlaps_common_windows.dir/binarized_narrow_broad_peaks_overlap_common_windows_matrix.gz")
def group_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows(infiles, outfile):
    ''' Groups the binarized overlaps of all samples broad and narrow peaks 
    with the constant windows into one matrix '''
     
    # Reuse the function inputting constant genomic windows and overlaps with 
    # narrow and broad peaks
    group_normalized_overlap_sample_with_common_peaks(infiles, outfile)






@follows(mkdir("n_overlaps_common_peaks.dir"))
@transform(group_normalized_overlap_sample_with_common_peaks,
           formatter("n_overlaps_common_peaks.dir/(?P<SAMPLE>.+?).gz"),
           "n_overlaps_common_peaks.dir/{SAMPLE[0]}_different_rows.tsv")
def delete_equal_rows_normalized_overlap_sample_with_common_peaks(infile, outfile):        
    ''' Deletes rows which contain the same values, shortens the header
    of the file to eliminate common prefixes and suffixes to all cell lines.
    '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create a temporal directory name for the run to make sure full execution 
    # before creating the outfile
    output_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = os.path.join(output_temp_dir, os.path.basename(outfile))
    
    # First eliminate rows which contain the same values
    statement = pipelineAtacseq.deleteEqualBedRowsStatement(infile, outfile_temp)
    
    P.run()
    
    # Shorten the headers of the cell lines to avoid errors in showing the plots
    # Modifies the provided file
    statement = pipelineAtacseq.shortenBedHeaderStatement(outfile_temp)
    
    P.run()
    
    # Move everything to the output directory, if everything goes well touch the outfile
    statement = ''' mv %(outfile_temp)s %(outfile)s; 
                    touch %(outfile)s;
    '''
    
    P.run()
    





# Analogous to delete_equal_rows_normalized_overlap_sample_with_common_peaks
@follows(mkdir("n_overlaps_common_windows.dir"))
@transform(group_normalized_overlap_sample_with_windows,
           formatter("n_overlaps_common_windows.dir/(?P<SAMPLE>.+?).gz"),
           "n_overlaps_common_windows.dir/{SAMPLE[0]}_different_rows.tsv")
def delete_equal_rows_normalized_overlap_sample_with_common_windows(infile, outfile): 
    ''' Deletes rows which contain the same values, shortens the header
    of the file to eliminate common prefixes and suffixes to all cell lines.
    '''
    
    # Reuse the function inputting constant genomic windows instead of peaks 
    delete_equal_rows_normalized_overlap_sample_with_common_peaks(infile, outfile)
    
    
    
    



# Analogous to delete_equal_rows_normalized_overlap_sample_with_common_peaks
@follows(mkdir("n_broad_narrow_peaks_overlaps_common_windows.dir"))
@transform(group_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows,
           formatter("n_broad_narrow_peaks_overlaps_common_windows.dir/(?P<SAMPLE>.+?).gz"),
           "n_broad_narrow_peaks_overlaps_common_windows.dir/{SAMPLE[0]}_different_rows.tsv")
def delete_equal_rows_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows(infile, outfile): 
    ''' Deletes rows which contain the same values, shortens the header
    of the file to eliminate common prefixes and suffixes to all cell lines.
    '''
    
    # Reuse the function inputting constant genomic windows instead of peaks 
    delete_equal_rows_normalized_overlap_sample_with_common_peaks(infile, outfile)
   




@transform(delete_equal_rows_normalized_overlap_sample_with_common_peaks,
           formatter("(?P<DIR>.+?)/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "{DIR[0]}/{SAMPLE[0]}_excluded_features_matrix_different_rows.tsv")
def remove_features_sample_with_common_peaks_before_clusterings(infile, outfile):
    '''Overlaps provided bed files with the genomic features used 
    for the clustering (such as windows or peaks) and removes any
    features which overlap the file.'''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Get the file extension of the infile, we need to input the same
    # extension to be processed correctly
    _, file_extension = os.path.splitext(infile)
    
    
    # Temp file to store everything (bed format) but the header to intesect
    # The extension has to be 
    headerless_infile = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name + file_extension
    
    header = ""
    
    
    # The input table with all the features contains a header, first extract the header
    with open(infile, 'r') as reader:
        
        header = reader.readline()
        
        header = header.rstrip('\n')
        
        # Store the fields as a list of strings
        # If not when outputing, it fails to output the tabs
        # in the statement
        header = header.split("\t")
        
    reader.close()
    
    
    # First copy the infile and delete the first line of the file
    statement = '''cp %(infile)s %(headerless_infile)s;
                    checkpoint;
                    
                    sed -i '1d' %(headerless_infile)s;
                    '''
    
    P.run()
    
    
    # Now exclude the regions from the list of bed format files
    excluded_beds = PARAMS["clustering_excluded_features"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    compressed_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    # Outputs a compressed file
    statement = pipelineAtacseq.createExcludingBedsFromBedStatement(headerless_infile, 
                                                                    excluded_beds, 
                                                                    compressed_temp)
    
    
    P.run()
    
    # Decompress the file, add the header to the created file
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp_decompressed = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
     
    # Then add the modified string with the header, output with \\t instead of \t
    # so that the statement substitution works fine
    statement = '''zcat %(compressed_temp)s > %(outfile_temp_decompressed)s;
                checkpoint;
                
                sed -i '1i'''+("\\t".join(header))+'''' %(outfile_temp_decompressed)s; 
                '''
    
    # Delete the temporal files and move to the outfile the processed one
    statement += '''rm %(headerless_infile)s %(compressed_temp)s;
                    checkpoint;
                    
                    mv %(outfile_temp_decompressed)s %(outfile)s;
                '''
    
    P.run()
    
    
    
    
# Analogous to remove_features_sample_with_common_peaks_before_clusterings but with sample reads in common windows    
@transform(delete_equal_rows_normalized_overlap_sample_with_common_windows,
           formatter("(?P<DIR>.+?)/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "{DIR[0]}/{SAMPLE[0]}_excluded_features_matrix_different_rows.tsv")    
def remove_features_sample_with_common_windows_before_clusterings(infile, outfile):
    '''Overlaps provided bed files with the genomic features used 
    for the clustering (such as windows or peaks) and removes any
    features which overlap the file.'''
    
    # Call the function
    remove_features_sample_with_common_peaks_before_clusterings(infile, outfile)
    
    
    
    
# Analogous to remove_features_sample_with_common_peaks_before_clusterings but with peaks in common windows    
@transform(delete_equal_rows_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows,
           formatter("(?P<DIR>.+?)/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "{DIR[0]}/{SAMPLE[0]}_excluded_features_matrix_different_rows.tsv")    
def remove_features_sample_with_peaks_overlap_sample_with_common_windows_before_clusterings(infile, outfile):
    '''Overlaps provided bed files with the genomic features used 
    for the clustering (such as windows or peaks) and removes any
    features which overlap the file.'''
    
    # Call the function
    remove_features_sample_with_common_peaks_before_clusterings(infile, outfile)
    
    

# @transform([delete_equal_rows_normalized_overlap_sample_with_common_peaks,
#             delete_equal_rows_normalized_overlap_sample_with_common_windows,
#             delete_equal_rows_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows],
#            formatter("(?P<DIR>.+?)/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
#            "{DIR[0]}/{SAMPLE[0]}_excluded_features_matrix_different_rows.tsv")
# def remove_features_before_clusterings(infile, outfile):
#     '''Overlaps provided bed files with the genomic features used 
#     for the clustering (such as windows or peaks) and removes any
#     features which overlap the file.'''
#     
#     # Get the temporal dir specified
#     tmp_dir = PARAMS["general_temporal_dir"]
#     
#     # Get the file extension of the infile, we need to input the same
#     # extension to be processed correctly
#     _, file_extension = os.path.splitext(infile)
#     
#     
#     # Temp file to store everything (bed format) but the header to intesect
#     # The extension has to be 
#     headerless_infile = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name + file_extension
#     
#     header = ""
#     
#     
#     # The input table with all the features contains a header, first extract the header
#     with open(infile, 'r') as reader:
#         
#         header = reader.readline()
#         
#         header = header.rstrip('\n')
#         
#         # Store the fields as a list of strings
#         # If not when outputing, it fails to output the tabs
#         # in the statement
#         header = header.split("\t")
#         
#     reader.close()
#     
#     
#     # First copy the infile and delete the first line of the file
#     statement = '''cp %(infile)s %(headerless_infile)s;
#                     checkpoint;
#                     
#                     sed -i '1d' %(headerless_infile)s;
#                     '''
#     
#     P.run()
#     
#     
#     # Now exclude the regions from the list of bed format files
#     excluded_beds = PARAMS["clustering_excluded_features"]
#     
#     # Create temp files to make sure the process doesn't create the files
#     # until it is finished 
#     compressed_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
#     
#     
#     # Outputs a compressed file
#     statement = pipelineAtacseq.createExcludingBedsFromBedStatement(headerless_infile, 
#                                                                     excluded_beds, 
#                                                                     compressed_temp)
#     
#     
#     P.run()
#     
#     # Decompress the file, add the header to the created file
#     # Create temp files to make sure the process doesn't create the files
#     # until it is finished 
#     outfile_temp_decompressed = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
#     
#      
#     # Then add the modified string with the header, output with \\t instead of \t
#     # so that the statement substitution works fine
#     statement = '''zcat %(compressed_temp)s > %(outfile_temp_decompressed)s;
#                 checkpoint;
#                 
#                 sed -i '1i'''+("\\t".join(header))+'''' %(outfile_temp_decompressed)s; 
#                 '''
#     
#     # Delete the temporal files and move to the outfile the processed one
#     statement += '''rm %(headerless_infile)s %(compressed_temp)s;
#                     checkpoint;
#                     
#                     mv %(outfile_temp_decompressed)s %(outfile)s;
#                 '''
#     
#     P.run()










# Cluster with removed and non-removed features
@transform([delete_equal_rows_normalized_overlap_sample_with_common_peaks,
            remove_features_sample_with_common_peaks_before_clusterings],
           formatter("n_overlaps_common_peaks.dir/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "n_overlaps_common_peaks.dir/{SAMPLE[0]}_clusters.png")
def cluster_normalized_overlap_sample_with_common_peaks(infile, outfile):
     
    '''Creates clusters of samples from the normalized overlaps 
    with the common peaks'''
     
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
     
    # Create a temporal directory name for the run to make sure full execution 
    # before creating the outfile
    output_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
     
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = os.path.join(output_temp_dir, os.path.basename(outfile))
     
    # Perform standard hierarchical clustering
    compseg.hierarchicalClustering(infile, outfile_temp)
     
     
    # Move everything back to the outfile directory
    output_dir = os.path.dirname(outfile)
     
    # Move everything to the output directory, if everything goes well touch the outfile
    statement = ''' mv %(output_temp_dir)s/* %(output_dir)s; 
                    touch %(outfile)s;
    '''
     
    P.run()




# Analogous to cluster_normalized_overlap_sample_with_common_peaks
# Cluster with removed and non-removed features
@transform([delete_equal_rows_normalized_overlap_sample_with_common_windows,
            remove_features_sample_with_common_windows_before_clusterings],
           formatter("n_overlaps_common_windows.dir/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "n_overlaps_common_windows.dir/{SAMPLE[0]}_clusters.png")
def cluster_normalized_overlap_sample_with_common_windows(infile, outfile):
         
    '''Creates clusters of samples from the normalized overlaps 
    with the constant windows'''
    
    cluster_normalized_overlap_sample_with_common_peaks(infile, outfile)



         


# Analogous to cluster_normalized_overlap_sample_with_common_peaks
# Cluster with removed and non-removed features
@transform([delete_equal_rows_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows,
            remove_features_sample_with_peaks_overlap_sample_with_common_windows_before_clusterings],
           formatter("n_broad_narrow_peaks_overlaps_common_windows.dir/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "n_broad_narrow_peaks_overlaps_common_windows.dir/{SAMPLE[0]}_clusters.png")
def cluster_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows(infile, outfile):
         
    '''Creates clusters of samples from the binarized overlaps of narrow and
    broad peaks with the constant windows '''
    
    cluster_normalized_overlap_sample_with_common_peaks(infile, outfile)




# Bootstrapping the clustering with more than 300.000 rows is an overkill
def filterTopVarianceRows(infile, outfile):
    ''' Orders the rows by variance and picks the top 300.000 rows. '''
    
    # The maximum number of rows
    num_max_rows = 300000
    
    
    # Calculate the number of lines in the file
    with open(infile) as f:
        for i, _ in enumerate(f):
            pass
    
    f.close()
      
    number_of_lines = i + 1
    
    # The file includes a header when counting the number of lines
    # If the number of lines is below the max number of lines, just copy
    if ((number_of_lines-1) <= num_max_rows):
        
        statement = '''cp %(infile)s %(outfile)s '''
        
        P.run()
    
    
    # Otherwise get the top num_max_rows by variance
    else:
    
        # Get the temporal dir specified
        tmp_dir = PARAMS["general_temporal_dir"]
        
        # Create temp files to make sure the process doesn't create the files
        # until it is finished 
        outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
        
        # The first 3 columns are chr, start and end (0-2)
        start_data_col = 3
        
        # The last data column is the last column in the dataframe
        end_data_col = -1
        
        # Get the number of tab separated fields
        # First determine the number of fields
        with IOTools.openFile(infile, "r") as reader:
            
            for line in reader:
                 
                # Remove the new line character from the line (avoids taking this into account downstream)
                line = line.rstrip('\n')
                 
                header_fields = line.split("\t")
                
                # The field numbering begins in 0
                end_data_col = len(header_fields) -1
                
                break
        
        reader.close()
        
        
        
        
        pipelineAtacseq.getTopVarianceRows(infile, 
                                           outfile_temp, 
                                           start_data_col, 
                                           end_data_col, 
                                           num_max_rows,
                                           submit=True,
                                           job_memory="16G")
    
    

        # Move the temp file to the outfile
        statement = ''' mv %(outfile_temp)s %(outfile)s; 
        '''
        
        P.run()
        

# Cluster with removed and non-removed features
@transform([delete_equal_rows_normalized_overlap_sample_with_common_peaks,
            remove_features_sample_with_common_peaks_before_clusterings],
           formatter("n_overlaps_common_peaks.dir/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "n_overlaps_common_peaks.dir/{SAMPLE[0]}_matrix_different_rows_top_variance.tsv")
def filterTopVarianceRowsCommonPeaks(infile, outfile):
    ''' Filters the top 300,000 rows based on variability ''' 
    
    # Reuses the function with the files specified
    filterTopVarianceRows(infile, outfile)

        


# Cluster with removed and non-removed features
@transform([delete_equal_rows_normalized_overlap_sample_with_common_windows,
            remove_features_sample_with_common_windows_before_clusterings],
           formatter("n_overlaps_common_windows.dir/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "n_overlaps_common_windows.dir/{SAMPLE[0]}_matrix_different_rows_top_variance.tsv")         
def filterTopVarianceRowsCommonWindows(infile, outfile):
    ''' Filters the top 300,000 rows based on variability '''
    
    # Reuses the function with the files specified
    filterTopVarianceRows(infile, outfile)






# Bootstrapping the clustering with more than 300.000 rows is an overkill
def filterRandomSampleRows(infile, outfile):
    ''' Filters 300,000 random rows. '''
    
    # The maximum number of rows
    num_max_rows = 300000
    
    
    # Calculate the number of lines in the file
    with open(infile) as f:
        for i, _ in enumerate(f):
            pass
    
    f.close()
      
    number_of_lines = i + 1
    
    # The file includes a header when counting the number of lines
    # If the number of lines is below the max number of lines, just copy
    if ((number_of_lines-1) <= num_max_rows):
        
        statement = '''cp %(infile)s %(outfile)s '''
        
        P.run()
    
    
    # Otherwise get the num_max_rows randomly
    else:
    
        # Get the temporal dir specified
        tmp_dir = PARAMS["general_temporal_dir"]
        
        # Create temp files to make sure the process doesn't create the files
        # until it is finished 
        outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
        
        # The first 3 columns are chr, start and end (0-2)
        start_data_col = 3
        
        # The last data column is the last column in the dataframe
        end_data_col = -1
        
        # Get the number of tab separated fields
        # First determine the number of fields
        with IOTools.openFile(infile, "r") as reader:
            
            for line in reader:
                 
                # Remove the new line character from the line (avoids taking this into account downstream)
                line = line.rstrip('\n')
                 
                header_fields = line.split("\t")
                
                # The field numbering begins in 0
                end_data_col = len(header_fields) -1
                
                break
        
        reader.close()
        
        
        
        
        pipelineAtacseq.getRandomSampleRows(infile, 
                                           outfile_temp, 
                                           start_data_col, 
                                           end_data_col, 
                                           num_max_rows,
                                           submit=True,
                                           job_memory="8G")
    
    

        # Move the temp file to the outfile
        statement = ''' mv %(outfile_temp)s %(outfile)s; 
        '''
        
        P.run()





# Cluster with removed and non-removed features
@transform([delete_equal_rows_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows,
            remove_features_sample_with_peaks_overlap_sample_with_common_windows_before_clusterings],
           formatter("n_broad_narrow_peaks_overlaps_common_windows.dir/(?P<SAMPLE>.+?)_matrix_different_rows.tsv"),
           "n_broad_narrow_peaks_overlaps_common_windows.dir/{SAMPLE[0]}_matrix_different_rows_random_sample.tsv")         
def filterRandomSampleRowsBroadNarrowPeaksCommonWindows(infile, outfile):
    ''' Filters a random sample of 300,000 rows'''
     
    # Reuses the function with the files specified
    filterRandomSampleRows(infile, outfile)   









@follows(mkdir("n_overlaps_common_peaks.dir"))
@originate( ["n_overlaps_common_peaks.dir/%s.metric" % metric for metric in 
            PARAMS["bootstrap_clustering_metrics"].split(",")])                       
def generateMetricBootstrapClusterings(outfile):
    
    ''' Generates the metrics specified for Bootstrap clusterings '''
    statement = ''' touch %(outfile)s '''
    
    P.run()
    
   


    
@follows(mkdir("n_overlaps_common_peaks.dir"))
@originate( ["n_overlaps_common_peaks.dir/%s.linkage_method" % linkage_method for linkage_method in 
            PARAMS["bootstrap_clustering_linkage_methods"].split(",")])                       
def generateLinkageBootstrapClusterings(outfile):
    
    ''' Generates the linkages specified for Bootstrap clusterings '''
    statement = ''' touch %(outfile)s '''
    
    P.run()






    
@follows(mkdir("n_overlaps_common_peaks.dir"),
         cluster_normalized_overlap_sample_with_common_peaks)
@product(generateMetricBootstrapClusterings,
          formatter(".+/(?P<METRIC>.+).metric"),
          generateLinkageBootstrapClusterings,
          formatter(".+/(?P<LINKAGE_METHOD>.+).linkage_method"),
          filterTopVarianceRowsCommonPeaks,
          formatter("n_overlaps_common_peaks.dir/(?P<SAMPLE>.+?)_matrix_different_rows_top_variance.tsv"),
          "n_overlaps_common_peaks.dir/{SAMPLE[2][0]}_bootstrap_{METRIC[0][0]}_{LINKAGE_METHOD[1][0]}.png",
          "{METRIC[0][0]}", "{LINKAGE_METHOD[1][0]}", "{SAMPLE[2][0]}")
def bootstrap_cluster_normalized_overlap_sample_with_common_peaks(infiles, outfile, metric, linkage_method, sample):   
    ''' Clusters with bootstrap using top variance (since we have non-binary features)'''
     
    infile_matrix = infiles[2]
     
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
      
    # Create a temporal directory name for the run to make sure full execution 
    # before creating the outfile
    output_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
      
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = os.path.join(output_temp_dir, os.path.basename(outfile))
                            
    
    
    # Get additional labels
    additional_labels = PARAMS["bootstrap_clustering_samples_labels_file"]
    
    # Will point to the infile (after header change if necessary)
    temp_infile_header_change = ""
    
    # If there are headers to add
    if additional_labels != "":
        
        # Create temporal file for header change
        temp_infile_header_change = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
        
        # Copy the infile matrix
        statement = ''' cp %(infile_matrix)s %(temp_infile_header_change)s;
        '''
        
        P.run()
        
        # Add the text
        statement = pipelineAtacseq.addHeaderTextToSamples(temp_infile_header_change,
                                                           additional_labels)
        
        P.run()
        
    else:
        
        # If there hasn't been any change, the temp_infile_header_change is just the infile
        temp_infile_header_change = infile_matrix
     
      
    # Run pvcluster to get resampling and a metric on how good the clusters are
       
    # Define command and arguments
    # Iceberg/Sharc compatible
    Rscript = '''machine_name=$(hostname);
                if [[ $machine_name == "sharc-node"* ]]; then 
                  machine_name="sharc";
                else 
                  module load apps/R/3.3.1;
                fi;
                
                Rscript'''
       
    path2script = '/home/mbp15ja/dev/AuxiliaryPrograms/Segmentations/Hierarchical_clustering.R'
   
    # Variable number of args in a list
    # call the function with argument
    args = [temp_infile_header_change, outfile_temp, "1000", metric, linkage_method]
   
    # Build subprocess command
    statement = " ".join([Rscript, path2script, ""])
    for arg in args:
        statement += arg+" "
           
    job_memory = "4G"
       
    P.run()
       
       
    # Move everything back to the outfile directory
    output_dir = os.path.dirname(outfile)
       
    # Move everything to the output directory, if everything goes well touch the outfile
    statement = ''' mv %(output_temp_dir)s/* %(output_dir)s; 
                    touch %(outfile)s;
    '''
       
    P.run()





# Analogous to bootstrap_cluster_normalized_overlap_sample_with_common_peaks
@follows(mkdir("n_overlaps_common_windows.dir"),
         cluster_normalized_overlap_sample_with_common_windows)
@product(generateMetricBootstrapClusterings,
          formatter(".+/(?P<METRIC>.+).metric"),
          generateLinkageBootstrapClusterings,
          formatter(".+/(?P<LINKAGE_METHOD>.+).linkage_method"),
          filterTopVarianceRowsCommonWindows,
          formatter("n_overlaps_common_windows.dir/(?P<SAMPLE>.+?)_matrix_different_rows_top_variance.tsv"),
          "n_overlaps_common_windows.dir/{SAMPLE[2][0]}_bootstrap_{METRIC[0][0]}_{LINKAGE_METHOD[1][0]}.png",
          "{METRIC[0][0]}", "{LINKAGE_METHOD[1][0]}", "{SAMPLE[2][0]}")
def bootstrap_cluster_normalized_overlap_sample_with_common_windows(infiles, outfile, metric, linkage_method, sample):
    ''' Clusters with bootstrap '''
    
    # Reuse the function inputting constant genomic windows instead of peaks
    bootstrap_cluster_normalized_overlap_sample_with_common_peaks(infiles, outfile, metric, linkage_method, sample)
    
    





#Analogous to bootstrap_cluster_normalized_overlap_sample_with_common_peaks
@follows(cluster_binarized_sample_broad_narrow_peaks_overlap_sample_with_common_windows)
@product(generateMetricBootstrapClusterings,
          formatter(".+/(?P<METRIC>.+).metric"),
          generateLinkageBootstrapClusterings,
          formatter(".+/(?P<LINKAGE_METHOD>.+).linkage_method"),
          filterRandomSampleRowsBroadNarrowPeaksCommonWindows,
          formatter("n_broad_narrow_peaks_overlaps_common_windows.dir/(?P<SAMPLE>.+?)_matrix_different_rows_random_sample.tsv"),
          "n_broad_narrow_peaks_overlaps_common_windows.dir/{SAMPLE[2][0]}_bootstrap_{METRIC[0][0]}_{LINKAGE_METHOD[1][0]}.png",
          "{METRIC[0][0]}", "{LINKAGE_METHOD[1][0]}", "{SAMPLE[2][0]}")
def bootstrap_cluster_binarized_sample_broad_narrow_peaks_overlap_with_common_windows(infiles, outfile, metric, linkage_method, sample):
    ''' Clusters with bootstrap, uses random sampling of features for binary features. '''
     
    # Reuse the function inputting constant genomic windows instead of peaks
    bootstrap_cluster_normalized_overlap_sample_with_common_peaks(infiles, outfile, metric, linkage_method, sample)
   



# Orders the matrix_input_file by variability in the features.
# Downsamples by half each time the number of features provided
# until it gets to the minimum. For each iteration:
# 1) It performs a bootstrap clustering with the parameters specified.
# 2) Checks to see if the boostrap clustering produces the same clusters
# as the ones provided.
# 3) If any cluster changes during the process it stops and reports
# the name of the file containing the matrix with the minimum downsample
# in features (taking into account variability) which still maintains
# the clustering.
# 
# Inputs:
#     -infiles: [starting_cluster_file, matrix_input_file]
#         -matrix_input_file: A matrix containing the features and labels.
#            Format (includes header):
#         
#         chrom   start   end     sample1       sample2
#         chr1    0       200     0.52      1.42
#         chr1    200     400     1.44      0.0012
#         chr1    400     600     12.45     5.122
#
#         -starting_cluster_file: tab separated list with all the elements belonging to one cluster in each line:
#           element1_cluster1 element2_cluster1 element3_cluster1
#           element1_cluster2 element2_cluster2 element3_cluster2
#
#     -outfile: The outfile containing the matrix_input_file with reduced number of rows
#        while still maintaining the clusterings.
#
#     -pvclust_metric: The metric to use with pvclust
#     -pvclust_linkage_method: The linkage method to use with the pvclust
#     -log_file: File to store the log information
def findMinFeaturesMaintainingBootstrapClustering(infiles, 
                                                  outfile, 
                                                  pvclust_metric, 
                                                  pvclust_linkage_method, 
                                                  log_file):
    ''' Finds the least number of features up to 10000 which maintain 
    the bootstrap significant clusters '''
    
    starting_cluster_file, matrix_input_file = infiles
    
    minimum_testing_features = 10000
    
    # The first 3 columns are chr, start and end (0-2)
    start_data_col = 3
    
    # The last data column is the last column in the dataframe
    end_data_col = -1
    
    # Get the number of tab separated fields
    # First determine the number of fields
    with IOTools.openFile(matrix_input_file, "r") as reader:
        
        for line in reader:
             
            # Remove the new line character from the line (avoids taking this into account downstream)
            line = line.rstrip('\n')
             
            header_fields = line.split("\t")
            
            # The field numbering begins in 0
            end_data_col = len(header_fields) -1
            
            break
    
    reader.close()
    
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Get the result
    [successful_clustering_feature_number,
     successful_clustering_matrix_filename] = clusterings.findMinFeaturesMaintainingPVClustering(minimum_testing_features,
                                       matrix_input_file,
                                       starting_cluster_file,
                                       pvclust_linkage_method,
                                       pvclust_metric,
                                       start_data_col,
                                       end_data_col,
                                       log_file,
                                       tmp_dir)
    
    
    # Move the result to the outfile and append to the log with the number of
    # features successfully clustering

    
    # If there wasn't any downsampling in features successful just copy the file
    # The files are deleted by the previous algorithm if no downsampling is possible
    if successful_clustering_matrix_filename == None:
        
        # We put \n before to make sure this is outputted in a new line
        # (the last line of the file)
        statement = '''cp %(matrix_input_file)s %(outfile)s;
                    printf "\\nOriginal features" >> %(log_file)s '''
        
    else:
        
        statement = '''mv %(successful_clustering_matrix_filename)s %(outfile)s;
                    printf "\\n%(successful_clustering_feature_number)s features" >> %(log_file)s; '''
        
        # If at least one downsampling succeeded, all the temporal files created
        # will remain. After copying the wanted files delete all the temporal
        # files left.
        clustering_temp_dir = os.path.dirname(successful_clustering_matrix_filename)
    
        statement += ''' rm -rf %(clustering_temp_dir)s; '''
    
    
        
    P.run()
    




    
# Finds the least number of features up to 10000 which maintain the bootstrap
# significant clusters 
def min_features_bootstrap_cluster_calculation(infiles, outfile, metric, linkage_method, sample):
    
    
    # Create a log file
    log_file = outfile + ".log"
    
    findMinFeaturesMaintainingBootstrapClustering(infiles,
                                                  outfile,
                                                  metric,
                                                  linkage_method,
                                                  log_file)
    
    

@follows(mkdir("n_overlaps_common_peaks_min_features.dir"),
         filterTopVarianceRowsCommonPeaks,
         bootstrap_cluster_normalized_overlap_sample_with_common_peaks)
@transform("n_overlaps_common_peaks.dir/*_bootstrap_*.tsv",
          formatter("n_overlaps_common_peaks.dir/(?P<SAMPLE>.+?)_bootstrap_(?P<METRIC>.+?)_(?P<LINKAGE_METHOD>.+?).tsv"), # Add the cluster information file
          add_inputs("n_overlaps_common_peaks.dir/{SAMPLE[0]}_matrix_different_rows_top_variance.tsv"), # Add the matrix with features 
          "n_overlaps_common_peaks_min_features.dir/min_features_bootstrap_{SAMPLE[0]}_{METRIC[0]}_{LINKAGE_METHOD[0]}.tsv",
          "{METRIC[0]}", "{LINKAGE_METHOD[0]}", "{SAMPLE[0]}")
def min_features_bootstrap_cluster_normalized_overlap_sample_with_common_peaks(infiles, outfile, metric, linkage_method, sample):
    ''' Finds the least number of features up to 10000 which maintain 
    the bootstrap significant clusters '''
    
    # Reuses the general method
    min_features_bootstrap_cluster_calculation(infiles, outfile, metric, linkage_method, sample)
    




@follows(mkdir("n_overlaps_common_windows_min_features.dir"),
         filterTopVarianceRowsCommonWindows,
         bootstrap_cluster_normalized_overlap_sample_with_common_windows)
@transform("n_overlaps_common_windows.dir/*_bootstrap_*.tsv",
          formatter("n_overlaps_common_windows.dir/(?P<SAMPLE>.+?)_bootstrap_(?P<METRIC>.+?)_(?P<LINKAGE_METHOD>.+?).tsv"), # Add the cluster information file
          add_inputs("n_overlaps_common_windows.dir/{SAMPLE[0]}_matrix_different_rows_top_variance.tsv"), # Add the matrix with features 
          "n_overlaps_common_windows_min_features.dir/min_features_bootstrap_{SAMPLE[0]}_{METRIC[0]}_{LINKAGE_METHOD[0]}.tsv",
          "{METRIC[0]}", "{LINKAGE_METHOD[0]}", "{SAMPLE[0]}")
def min_features_bootstrap_cluster_normalized_overlap_sample_with_common_windows(infiles, outfile, metric, linkage_method, sample):
    ''' Finds the least number of features up to 10000 which maintain 
    the bootstrap significant clusters '''
     
    # Reuses the general method
    min_features_bootstrap_cluster_calculation(infiles, outfile, metric, linkage_method, sample)
    
    

    


@transform([min_features_bootstrap_cluster_normalized_overlap_sample_with_common_peaks,
            min_features_bootstrap_cluster_normalized_overlap_sample_with_common_windows],
         formatter("(?P<DIR>.+)/min_features_bootstrap_(?P<SAMPLE>.+)_(?P<METRIC>.+)_(?P<LINKAGE_METHOD>.+).tsv"),
         "{DIR[0]}/min_features_bootstrap_{SAMPLE[0]}_{METRIC[0]}_{LINKAGE_METHOD[0]}.png",
         "{METRIC[0]}", "{LINKAGE_METHOD[0]}", "{SAMPLE[0]}")
def plot_heatmap_with_dendrograms(infile, outfile, metric, linkage_method, sample):
    ''' Plots the heatmaps of samples (cell lines) with the features 
    (normalized reads in peaks and windows) and the 
    corresponding dendrograms. To avoid problematic plottings only 
    when the minimum number of features are attained for the downsampling
    of features, the plots are generated'''
     
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
      
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp = os.path.join(tmp_dir, os.path.basename(outfile))
                            
     
    # Decide if the minimum number of features are attained from the
    # end of the log file. The minimum number of features to compare to
    # should be the same as findMinFeaturesMaintainingBootstrapClustering
    minimum_testing_features = 10000
    
    # The format of the last line is "Number features" or "Original features"
    features_from_file = None
    
    # The log file to parse for the number of features is the infile + .log
    log_file = infile + ".log"
    
    with open(log_file) as reader:
        for line in reader:
            
            features_from_file = line.rstrip('\n')
    
    
    # Create the statement variable empty
    statement = ""
    
    # If the features haven't retained clusters at the minimum features just 
    # touch the outfile
    if features_from_file.split(" ")[0] != str(minimum_testing_features):
        
        statement = "touch %(outfile_temp)s;"
        
    else:        
        # Run biclustering_with_dendrogram
        
        # Get additional labels
        additional_labels = PARAMS["bootstrap_clustering_samples_labels_file"]
        
        # Will point to the infile (after header change if necessary)
        temp_infile_header_change = ""
        
        # If there are headers to add
        if additional_labels != "":
            
            # Create temporal file for header change
            temp_infile_header_change = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
            
            # Copy the infile matrix
            statement = ''' cp %(infile)s %(temp_infile_header_change)s;
            '''
            
            P.run()
            
            # Add the text
            statement = pipelineAtacseq.addHeaderTextToSamples(temp_infile_header_change,
                                                               additional_labels)
            
            P.run()
            
        else:
            
            # If there hasn't been any change, the temp_infile_header_change is just the infile
            temp_infile_header_change = infile
        
        
           
        # Define command and arguments
        # Iceberg/Sharc compatible
        Rscript = '''machine_name=$(hostname); 
                if [[ $machine_name == "sharc-node"* ]]; then 
                  machine_name="sharc"; 
                else 
                  module load apps/R/3.3.1;
                fi;
                
                Rscript'''
           
        path2script = '/home/mbp15ja/dev/AuxiliaryPrograms/Segmentations/biclustering_with_dendrogram.R'
       
        # Variable number of args in a list
        # call the function with argument
        args = [temp_infile_header_change, outfile_temp, metric, linkage_method]
       
        # Build subprocess command
        statement = " ".join([Rscript, path2script, ""])
        for arg in args:
            statement += arg+" "
               
        job_memory = "4G"
       
    P.run()
       
       
    # Move everything to the output directory, if everything goes well touch the outfile
    statement = ''' mv %(outfile_temp)s %(outfile)s
    '''
       
    P.run()





@follows(bootstrap_cluster_normalized_overlap_sample_with_common_peaks)
def clustering_all_samples_overlap_common_peaks():
    ''' Dummy task to sync clustering of all samples reads overlapping common peaks '''
    pass




@follows(bootstrap_cluster_normalized_overlap_sample_with_common_windows)
def clustering_all_samples_overlap_common_windows():
    ''' Dummy task to sync clustering of all samples reads overlapping common windows '''
    pass



@follows(bootstrap_cluster_binarized_sample_broad_narrow_peaks_overlap_with_common_windows)
def clustering_sample_broad_narrow_peaks_overlap_common_windows():
    ''' Dummy task to sync clustering of all samples reads overlapping common windows '''
    pass



@follows(clustering_all_samples_overlap_common_peaks,
         clustering_all_samples_overlap_common_windows,
         plot_heatmap_with_dendrograms)
def typical_clustering_tasks():
    ''' Dummy task to allow only the execution of clustering_all_samples_overlap_common_peaks
    and clustering_all_samples_overlap_common_windows '''
    pass

@follows(typical_clustering_tasks,
         clustering_sample_broad_narrow_peaks_overlap_common_windows)
def clusteringTasks():
    ''' Dummy task for all clustering tasks '''
    pass



####################### Sample peaks in common peaks #######################

# Analogous to overlap_sample_with_common_peaks
@follows(mkdir("common_peaks_overlap_sample_peaks.dir"))
@transform(merge_all_peaks_per_sample,
           regex(r".+\.dir/(?!pooled_[control|treatment])(.+)_merged_narrow_broad_peaks.gz"),
           add_inputs(merge_all_peaks),
           r"common_peaks_overlap_sample_peaks.dir/\1_merged_narrow_broad_peaks_overlap_merged_all_samples_peaks.gz")
def sample_broad_narrow_peaks_overlap_merged_all_samples_peaks(infiles, outfile):
    ''' For each merged peak union for all samples, it calculates the number of merged narrow and
    broad peaks from the sample which fall in that region. Each sample peak will be completely included
    in all the samples peaks merged, therefore the overlaps will always be 100% and there is no need
    to use a fraction overlap in terms of each sample peak. '''
    
    # Reuse the function inputting constant all the merged peaks from all the samples
    # and the sample merged narrow and broad peaks. Note that any sample peak will be
    # completely included in all samples merged peaks (merge_all_peaks), the overlap will
    # be 100% always and there is no need to use a fraction overlap.
    overlap_sample_with_common_peaks(infiles, outfile)




# Analogous to binarize_sample_broad_narrow_peaks_overlap_sample_with_common_windows
@transform(sample_broad_narrow_peaks_overlap_merged_all_samples_peaks,
           regex(r"common_peaks_overlap_sample_peaks.dir/(?!pooled_[control|treatment])(.+)_merged_narrow_broad_peaks_overlap_merged_all_samples_peaks.gz"),
           r"common_peaks_overlap_sample_peaks.dir/\1_binarized_merged_narrow_broad_peaks_overlap_merged_all_samples_peaks.gz")
def binarize_sample_broad_narrow_peaks_overlap_merged_all_samples_peaks(infile, outfile):
    
    ''' For each sample, it binarizes the number of merged narrow and broad peaks
    overlapping each common peak to all samples '''
    
    # Reuse binarize_sample_broad_narrow_peaks_overlap_sample_with_common_windows
    # Inputting the sample peaks overlapping each merged common peak
    binarize_sample_broad_narrow_peaks_overlap_sample_with_common_windows(infile, outfile)



# Analogous to group_normalized_overlap_sample_with_common_peaks
@merge(binarize_sample_broad_narrow_peaks_overlap_merged_all_samples_peaks,
       "common_peaks_overlap_sample_peaks.dir/overlap_binary_common_peaks_matrix.gz")
def group_binary_sample_broad_narrow_peaks_overlap_merged_all_samples_peaks(infiles, outfile):
    ''' Groups the binary overlaps of each samples peaks with the merged peaks from all the samples into one matrix '''
    
    # Reuse the function inputting constant genomic windows instead of peaks
    group_normalized_overlap_sample_with_common_peaks(infiles, outfile)




# Analogous to delete_equal_rows_normalized_overlap_sample_with_common_peaks
@transform(group_binary_sample_broad_narrow_peaks_overlap_merged_all_samples_peaks,
           formatter("common_peaks_overlap_sample_peaks.dir/(?P<SAMPLE>.+?).gz"),
           "common_peaks_overlap_sample_peaks.dir/{SAMPLE[0]}_different_rows.tsv")
def delete_equal_rows_binary_sample_broad_narrow_peaks_overlap_merged_all_samples_peaks(infile, outfile): 
    ''' Deletes rows which contain the same values, shortens the header
    of the file to eliminate common prefixes and suffixes to all cell lines.
    '''
    
    # Reuse the function inputting constant common merged peaks 
    delete_equal_rows_normalized_overlap_sample_with_common_peaks(infile, outfile)



####################### Sample peaks in common peaks #######################
                    
                    
                    
                    
####################### Generate shifted filtered single ends #######################
@follows(mkdir("filtered_tag_align.dir"))
@transform(shiftTagAlign,
           formatter(".+\.dir/(?P<SAMPLE>(?!pooled_[control|treatment]).+)\.PE2SE\.tn5_shifted\.tagAlign\.gz"),
           "filtered_tag_align.dir/{SAMPLE[0]}.single.end.shifted.filtered.tagAlign.gz")
def filterShiftTagAlign(infile, outfile):

    ''' Filters out regions of low mappability and excessive mappability in the shifted single ends'''
    
    # Reuse the filter_peaks function
    filter_peaks(infile, outfile)

####################### Generate shifted filtered single ends #######################                 


@active_if(PARAMS["samples_pool_condition"] == 1)        
@follows(mkdir("naive_overlap_peaks.dir"))
@collate(filter_peaks,
         formatter(".+\.dir/.+_peaks\.(?P<EXTENSION>(narrowPeak))\.gz"),
         add_inputs("samples.tsv"),
         ["naive_overlap_peaks.dir/control.{EXTENSION[0]}.gz",
          "naive_overlap_peaks.dir/treatment.{EXTENSION[0]}.gz"])
def naive_overlap_thresholding_peaks(infiles, outfiles):
    ''' Performs naive overlap thresholding: peaks in the pooled data 
    (reads pooled across reps) that overlap peaks in ALL true replicates.
    The pooled control narrow peaks will be gotten from:
    filter_peaks.dir/pooled_control_peaks.narrowPeak.gz
    The pooled treatment: filter_peaks.dir/pooled_treatment_peaks.narrowPeak.gz'''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp_control = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    outfile_temp_treatment = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    # Separate the narrowPeak files from samples.tsv
    # infiles is 
    #[[narrowPeak.gz file, samples.tsv], 
    # [narrowPeak.gz file, samples.tsv]]
    
    narrow_peak_files = []
    
    # Get the shifted tag align file
    samples_table = infiles[0][1]
    
    # Get the narrowPeak.gz files    
    for infile_array in infiles:
        
        # Append the first file of the array (narrowPeak.gz) to narrow_peak_files
        narrow_peak_files.append(infile_array[0])
    
    
    # Define the other variables to create the statement
    output_control = outfiles[0]
    
    output_treatment = outfiles[1]
    
    sample_files_directory = os.path.dirname(infiles[0][0])
    
    # The samples_table contains the name of the samples, but a suffix
    # has to be added to get from samples to narrow peaks
    file_suffix = "_peaks.narrowPeak.gz"
    
    statement = pipelineAtacseq.createNaiveOverlapThresholdingPeaksStatement(samples_table,
                                                 narrow_peak_files,
                                                 outfile_temp_control,
                                                 outfile_temp_treatment,
                                                 file_suffix,
                                                 sample_files_directory)

    P.run()
    
    # Now move the files to the appropiate directory
    # Touch temp files in case they don't exist
    statement = ''' touch %(outfile_temp_control)s;
                    touch %(outfile_temp_treatment)s;
                    
                    mv %(outfile_temp_control)s %(output_control)s;
                    checkpoint;
                    
                    mv %(outfile_temp_treatment)s %(output_treatment)s;
                    '''
    
    P.run()
    
        
    
    
    
    
    
    
@follows(mkdir("n_peaks.dir"), call_peaks)    
@collate([filter_peaks, 
            "peaks_narrow.dir/*_peaks.narrowPeak.gz", 
            "peaks_broad.dir/*_peaks.broadPeak.gz"],
           formatter(".+\.dir/(?P<SAMPLE>.+)_peaks\.(?P<EXTENSION>(narrowPeak|broadPeak))\.gz"),
           "n_peaks.dir/{SAMPLE[0]}.tsv.gz")
def calculateNumberOfPeaks(infiles, outfile):
    '''Calculates number of narrow and broad peaks before and after filtering'''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]    
    
    statement = pipelineAtacseq.reportPeaks(infiles, outfile, tmp_dir)
    
    P.run()
    
    
    
    
@follows(index,
         calculateLibrarycomplexity,
         markDuplicates,
         calculateNumberOfPeaks,
         calculateNumberOfSingleEnds,
         calculate_sample_assigned_fraction)    
def sync_stats():
    '''Dummy task to sync all the stats generation'''
    

@follows(mkdir("stats.dir"),
         sync_stats)
@transform(index,
           regex(".+/(.+).flagstats"),
           add_inputs([r"library_complexity.dir/\1.pbc.qc",
                       r"dedupped.dir/\1.bam.metrics",
                       r"n_peaks.dir/\1.tsv.gz",
                       r"n_reads.dir/\1_single_end_reads.tsv",
                       r"assigned_fraction.dir/\1.assigned_fraction.tsv"]),
           r"stats.dir/\1.grouped.tsv")    
def group_stats_per_sample(infiles, outfile):
    ''' Gets all the stats into one table for each sample '''
    
    flagstats_file = infiles[0]
    library_complexity_file = infiles[1][0]
    mark_duplicates_file = infiles[1][1]
    number_peaks_file = infiles[1][2]
    number_reads_file = infiles[1][3]
    assigned_fraction_file = infiles[1][4]
    
    # Get the flagstats
    flagstats = logParser.flagstatsLogParser(flagstats_file)
    
    # Get the library complexity stats
    library_complexity_stats = logParser.parseGenericTabSepTable(library_complexity_file)
    
    # Get the duplicates stats
    mark_duplicates_stats = logParser.MarkDuplicatesLogParser(mark_duplicates_file)
    
    # Get the number of peaks stats
    number_peaks_stats = logParser.parseGenericTabSepTable(number_peaks_file, values_second_element="column")
    
    # Get the number reads stats
    number_reads_stats = logParser.readSingleNumberFile(number_reads_file)
    
    # Get the assigned fraction stats
    assigned_fraction_stats = logParser.readSingleFloatFile(assigned_fraction_file)
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
      
    # Create a temporal directory name for the run to make sure full execution 
    # before creating the outfile
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name 
      
    pipelineAtacseq.generateGroupStatsReport(flagstats,
                             library_complexity_stats,
                             mark_duplicates_stats,
                             number_peaks_stats,
                             number_reads_stats,
                             assigned_fraction_stats,
                             outfile_temp)
    
    
    statement = '''mv %(outfile_temp)s %(outfile)s;
                   touch %(outfile)s '''
    
    P.run()


@follows(mkdir("stats.dir"))
@merge(group_stats_per_sample,
       "stats.dir/experiment.stats.tsv")    
def group_stats_per_experiment(infiles, outfile):
    
    ''' Merges all the stats for each sample into one file. '''
    
    # Get a string space separated list of all the infiles
    all_infiles_string = " ".join(infiles)
    
    log_file = P.snip(outfile, ".tsv") + ".log"
    
    error_file = P.snip(outfile, ".tsv") + ".error"
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
      
    # Create a temporal directory name for the run to make sure full execution 
    # before creating the outfile
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name 
        
    statement = '''python %%(scriptsdir)s/combine_tables.py %(all_infiles_string)s 
                -c 1 -k 2 --add-file-prefix 
                --regex-filename="(.*).grouped.tsv" -L %(log_file)s -E %(error_file)s 
                 > %(outfile_temp)s;
                 checkpoint; ''' % locals()
    
    # Create the new header
    # Reduce the filenames in the headers by eliminating the longest common suffix and prefix
    minimum_filenames_header = StringOperations.shortenStringsByCommonPrefixAndSuffix(infiles)
    
    # First element variable
    first_element = []
    
    new_header = []
    
    # To complete the header we need the element in the first row first column
    with open(infiles[0], 'r') as reader:
        
        first_element.append((reader.readline()).split("\t")[0])
        
    reader.close()
    
    # Append the filenames
    new_header = first_element + minimum_filenames_header
    
    
    # First delete the first line of the file
    statement += '''sed -i '1d' '''+outfile_temp+'''; 
                    checkpoint;
                    '''
    
    # Then add the modified string with the header
    statement += '''sed -i '1i'''+("\\t".join(new_header))+'''' '''+outfile_temp+''';
                checkpoint; 
                '''
    
    statement += '''mv %(outfile_temp)s %(outfile)s;
                   touch %(outfile)s '''
    
    P.run()
    
    
    
@follows(mkdir("IGV.dir"),
         call_peaks)
@transform("peaks_narrow.dir/*.bdg",
         formatter(".+/(?P<SAMPLE>.+?)\_(?P<TYPE>(control_lambda|treat_pileup)).bdg$"),
         "IGV.dir/{SAMPLE[0]}_{TYPE[0]}.tdf")
def bdg2tdf_igv(infile, outfile):
    ''' Converts files from bdg to tdf to efficiently view them in IGV. 
    This task is only using the signal files generated from the call_peaks_narrow 
    (and not call_peaks_broad) using the additional --SPMR flag which normalizes 
    fragment pileup per million reads and it is comparable among different samples.'''
    
    # Get the genome name
    genome_name = PARAMS["general_genome"]
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    
    # Getting full directory where the pipeline is being executed, using the variable in parameters because it fails less
    dir_execution = PARAMS["workingdir"]
    
    # A log file igv.log will be created in the path where igv tools is being executed
    # Therefore, the program must be run in a temporary directory so the creation of
    # igv.log for different files is not overwritting the same file
    # We therefore need full paths
    full_path_infile = os.path.join(dir_execution, infile)
    
    full_path_outfile = os.path.join(dir_execution, outfile)
      
    # Create a temporal directory name for the run temporary files
    run_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    # Create temporary file to produce the full run on it, once finished, 
    # it is copied to the outfile. Add the .tdf extension
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name + ".tdf"
    
    
    # Log file
    log_file_full_path = P.snip(full_path_outfile, ".tdf") + ".log"
    
    # First we change to the temp directory
    # We perform the execution there, using the full path of the infile
    # Then we move the generated file back to the outfile
    # We change directory to the pipeline execution directory
    # Then we remove the temp directory created
    statement = '''cd %(run_temp_dir)s;
    
    igvtools toTDF -t %(run_temp_dir)s %(full_path_infile)s %(outfile_temp)s %(genome_name)s > %(log_file_full_path)s;
    checkpoint;
    
    mv %(outfile_temp)s %(full_path_outfile)s;
    checkpoint;
    
    cd %(dir_execution)s;
    
    rm -rf %(run_temp_dir)s;
    
    
    '''
    
    job_memory = "4G"
    
    P.run()



@follows(mkdir("IGV.dir"),
         call_peaks)
@transform(["peaks_narrow.dir/*_peaks.narrowPeak.gz",
            "peaks_broad.dir/*_peaks.broadPeak.gz",
            "peaks_broad.dir/*_peaks.gappedPeak.gz"],
           regex(".+/(.+)"),
           r"IGV.dir/\1")
def sort_peaks_igv(infile, outfile):
    ''' Sorts by coordinate the peaks '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temporary file to produce the full run on it, once finished, 
    # it is copied to the outfile.
    outfile_temp = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    statement = '''zcat %(infile)s | sort -k1,1 -k2,2n | gzip -c > %(outfile_temp)s;
    checkpoint;
    
    touch %(outfile_temp)s;
    mv %(outfile_temp)s %(outfile)s;
    '''
    
    P.run()





@follows(call_peaks) # To make sure we have the summits.bed
@transform([sort_peaks_igv,
            extend_five_prime_single_ends,
            "peaks_narrow.dir/*_summits.bed"],
         formatter("(?P<DIR>.+)/(?P<SAMPLE>.+?)?(\.bed|\.gz)?$"),
         "{DIR[0]}/{SAMPLE[0]}.bed.idx",
         "{DIR[0]}",
         "{SAMPLE[0]}")
def index_bed_like_igv(infile, outfile, dir, sample):
    ''' Indexes all the bed and bed-like files for IGV '''
    
    # In cases where the bed file is empty (for example for the pooled_control or
    # pooled_treatment having no sample), the indexing will give an error. In this case
    # We check before and just touch the outfile
    number_lines_file = file_operations.number_lines(infile)
    
    if number_lines_file == 0:
        
        statement = '''touch %(outfile)s;'''
        
        P.run()
    
    
    # If it's not empty, run normally    
    else:
    
        # The index will refer to the local file (we execute the indexing
        # from the same directory as the file to be indexed)
        outdir = os.path.dirname(outfile)
        
        # We need to output a .bed file to index, we are doing it from the
        # directory, so we only get the name without directory
        outfile_basename_bed = P.snip(os.path.basename(outfile), ".idx")
        
        infile_basename = os.path.basename(infile)
        
        log_file_basename = outfile_basename_bed + ".log"
        
        
        # Begin the creation of the statement, change directory to the directory
        # containing both the infile and outfile
        statement = '''cd %(outdir)s; '''
        
        # If the bed file is compressed, it has to be decompressed first
        if infile.endswith(".gz"):
            
            
            
            statement += ''' zcat %(infile_basename)s > %(outfile_basename_bed)s; 
            
            igvtools index %(outfile_basename_bed)s > %(log_file_basename)s;
            
            '''
        
        # If its not compressed and ends in .bed
        # (ie. summits.bed, just index, infile_temp = infile
        else: 
            # Index the file
            statement += '''igvtools index %(infile_basename)s > %(log_file_basename)s '''
        
        
        job_memory = "4G"
        
        P.run()



    




@follows(mkdir("bigWigs.dir"),
         call_peaks)
@collate("peaks_narrow.dir/*.bdg",
         formatter(".+/peaks_(?P<PEAK_TYPE>.+?)\.dir/(?P<SAMPLE>.+?)\_(control_lambda.bdg$|treat_pileup.bdg$.*?)"),
         "bigWigs.dir/{SAMPLE[0]}.{PEAK_TYPE[0]}.fc.signal.bw",
         "{SAMPLE[0]}", "{PEAK_TYPE[0]}")
def generateFCBigWigs(infiles, outfile, sample, peak_type):
    ''' Generates the Fold Change signal bigwig files.
    Note that they are generated on the called peaks, not the filtered peaks.'''
    
    # The pooled datasets can be empty if there are no samples in either
    # perform a check and if they are empty just touch the outfile
    empty_condition = False
    
    if (sample == "pooled_treatment" or
        sample == "pooled_control"):
        
        # Get the sample type (control or treatment)
        sample_type = sample[7:]
        
        # Get the samples table
        sample_details = PARAMS["samples_details_table"]
        
        # See if the condition is empty
        empty_condition = pipelineAtacseq.emptyConditionSamplesTable(sample_details,
                                                                     sample_type)
        
    
    if empty_condition:
        
        statement = "touch %(outfile)s"
        
        P.run()
        
    
    
    # If they are not empty, proceed as normal    
    else:
    
        control_lambda = ""
        treat_pileup = ""
        
        found_files = 0
        
        # Separate the control lambda (background signal file) from the signal file
        # MM020616.AGGCAGAA.bwa_pos_sorted_control_lambda.bdg           
        # MM020616.AGGCAGAA.bwa_pos_sorted_treat_pileup.bdg
        for infile in infiles:
            
            if infile.endswith("control_lambda.bdg"):
                
                control_lambda = infile
                
                found_files += 1
            
            elif infile.endswith("treat_pileup.bdg"):
                
                treat_pileup = infile
                
                found_files += 1
        
        # If not all files are found report an exception
        if found_files != 2:
            
            raise Exception ("The files required haven't been found")
        
        
        
        # Get the contigs
        contigs = PARAMS["general_contigs"]
        
        # Log file
        log_file = P.snip(outfile, ".signal.bw") + ".log"
        
        # Get the temporal dir specified
        tmp_dir = PARAMS["general_temporal_dir"]
        
        # Create a temporal directory name for the run to make sure full execution 
        # before creating the outfile
        bigwig_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
        
        # Outfile prefix
        outfile_prefix = P.snip(os.path.basename(outfile), ".fc.signal.bw")
        
        # Create a prefix name for the temp outfile
        temp_outfile_path_prefix = os.path.join(bigwig_temp_dir, outfile_prefix)
            
        
        job_memory="16G"
        
        
        # The pooled datasets contain a lot of reads, it is advisable to up
        # the memory in this case
        if (sample == "pooled_treatment" or
            sample == "pooled_control"):
            
            job_memory = "32G"
        
        
        statement = '''macs2 bdgcmp -t %(treat_pileup)s -c %(control_lambda)s 
        --o-prefix %(temp_outfile_path_prefix)s -m FE 2> %(log_file)s;
        
        '''
        
        P.run()
        
        
        # The outfile from the previous step will have this name
        temp_outfile_path = temp_outfile_path_prefix + "_FE.bdg"
        
        
        # To make sure the step finishes before creating the output file
        temp_outfile2 = temp_outfile_path_prefix + "_temp_FE.bdg"
        
        # Log file for the slopping correction
        log_file_slop_correction = P.snip(outfile, ".signal.bw") + ".correction.log"
        
        
        #Check that the slop does not surpass the chromosome edges and that the starts are < than ends
        PipelineChromHMM.correctSlopChromosomeEdges(temp_outfile_path, 
                                                    contigs,
                                                    temp_outfile2,
                                                    log_file_slop_correction,
                                                    submit=True,
                                                    job_memory="4G")
    
        
        # Sort the bedgraph and create a bigwig file
        sorted_bedgraph = temp_outfile_path_prefix + "_temp_FE_sorted.bdg"
        
        temporal_output_bigwig = temp_outfile_path_prefix + "temp.fc.signal.bw"
    
        job_memory="8G"
        
        # For the pooled samples, the sort command can run out of space if
        # the /tmp directory is used, specify another temp directory
        sort_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
        
        # Remove the temp file, sort and output bigwig
        # Sort command with 6G maximum buffer despite 8G assigned memory
        statement = ''' rm %(temp_outfile_path)s;
        checkpoint;
        
        sort -k1,1 -k2,2n -S 6G -T %(sort_temp_dir)s %(temp_outfile2)s > %(sorted_bedgraph)s;
        checkpoint;
        
        rm %(temp_outfile2)s;
        checkpoint;
        
        bedGraphToBigWig %(sorted_bedgraph)s %(contigs)s %(temporal_output_bigwig)s 2>> %(log_file)s;
        checkpoint;
        
        rm %(sorted_bedgraph)s;
        checkpoint;
        
        mv %(temporal_output_bigwig)s %(outfile)s '''
         
        P.run()



    


@follows(mkdir("bigWigs.dir"),
         call_peaks,
         shiftTagAlign)
@collate("peaks_narrow.dir/*.bdg",
         formatter(".+/peaks_(?P<PEAK_TYPE>.+?)\.dir/(?P<SAMPLE>.+?)\_(control_lambda.bdg$|treat_pileup.bdg$.*?)"),
         add_inputs("final_tag_align.dir/{SAMPLE[0]}.PE2SE.tn5_shifted.tagAlign.gz"),
         "bigWigs.dir/{SAMPLE[0]}.{PEAK_TYPE[0]}.pval.signal.bw",
         "{SAMPLE[0]}", "{PEAK_TYPE[0]}")
def generateLogPvalBigWigs(infiles, outfile, sample, peak_type):
    ''' Generates the -log10(pvalue) signal bigwig files.
    Note that they are generated on the called peaks, not the filtered peaks. '''
    
    # The pooled datasets can be empty if there are no samples in either
    # perform a check and if they are empty just touch the outfile
    empty_condition = False
    
    if (sample == "pooled_treatment" or
        sample == "pooled_control"):
        
        # Get the sample type (control or treatment)
        sample_type = sample[7:]
        
        # Get the samples table
        sample_details = PARAMS["samples_details_table"]
        
        # See if the condition is empty
        empty_condition = pipelineAtacseq.emptyConditionSamplesTable(sample_details,
                                                                     sample_type)
        
    
    if empty_condition:
        
        statement = "touch %(outfile)s"
        
        P.run()
        
    
    
    # If they are not empty, proceed as normal    
    else: 
    
        # infiles is 
        #[[.bdg file, tagAlign file], 
        # [.bdg file, tagAlign file]]
        
        control_lambda = ""
        treat_pileup = ""
        bdg_files = []
        
        # Get the shifted tag align file
        shifted_tag_align = infiles[0][1]
        
        # Get the .bdg files    
        for infile_array in infiles:
            # Append the first file of the array (.bdg) to bdg_files
            bdg_files.append(infile_array[0])
            
        
        
        
        # Separate .bdg files
        found_files = 0
        
        # Separate the control lambda (background signal file) from the signal file
        # MM020616.AGGCAGAA.bwa_pos_sorted_control_lambda.bdg           
        # MM020616.AGGCAGAA.bwa_pos_sorted_treat_pileup.bdg
        for bdg_file in bdg_files:
            
            if bdg_file.endswith("control_lambda.bdg"):
                
                control_lambda = bdg_file
                
                found_files += 1
            
            elif bdg_file.endswith("treat_pileup.bdg"):
                
                treat_pileup = bdg_file
                
                found_files += 1
        
        # If not all files are found report an exception
        if found_files != 2:
            
            raise Exception ("The files required haven't been found")
        
        
        
        # Get the contigs
        contigs = PARAMS["general_contigs"]
        
        # Log file
        log_file = P.snip(outfile, ".signal.bw") + ".log"
        
        # Get the temporal dir specified
        tmp_dir = PARAMS["general_temporal_dir"]
        
        # Create a temporal directory name for the run to make sure full execution 
        # before creating the outfile
        bigwig_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
        
        # Outfile prefix
        outfile_prefix = P.snip(os.path.basename(outfile), ".pval.signal.bw")
        
        # Create a prefix name for the temp outfile
        temp_outfile_path_prefix = os.path.join(bigwig_temp_dir, outfile_prefix)
            
        
        job_memory="16G"
        
        
        # The pooled datasets contain a lot of reads, it is advisable to up
        # the memory in this case
        if (sample == "pooled_treatment" or
            sample == "pooled_control"):
            
            job_memory = "32G"
        
        
        # sval counts the number of tags per million in the (compressed) BED file
        statement = '''sval=$(wc -l <(zcat -f %(shifted_tag_align)s) | 
        awk '{printf "%%f", $1/1000000}');
        checkpoint;   
        
        macs2 bdgcmp -t %(treat_pileup)s -c %(control_lambda)s 
        --o-prefix %(temp_outfile_path_prefix)s -m ppois -S "${sval}" 2> %(log_file)s
        '''
        
        P.run()
        
        
        # The outfile from the previous step will have this name
        temp_outfile_path = temp_outfile_path_prefix + "_ppois.bdg"
        
        
        # To make sure the step finishes before creating the output file
        temp_outfile2 = temp_outfile_path_prefix + "_temp_ppois.bdg"
        
        # Log file for the slopping correction
        log_file_slop_correction = P.snip(outfile, ".signal.bw") + ".correction.log"
        
        
        #Check that the slop does not surpass the chromosome edges and that the starts are < than ends
        PipelineChromHMM.correctSlopChromosomeEdges(temp_outfile_path, 
                                                    contigs,
                                                    temp_outfile2,
                                                    log_file_slop_correction,
                                                    submit=True,
                                                    job_memory="4G")
    
        
        # Sort the bedgraph and create a bigwig file
        sorted_bedgraph = temp_outfile_path_prefix + "_temp_ppois_sorted.bdg"
        
        temporal_output_bigwig = temp_outfile_path_prefix + "temp.pval.signal.bw"
    
        job_memory="8G"
        
        
        # For the pooled samples, the sort command can run out of space if
        # the /tmp directory is used, specify another temp directory
        sort_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
        
        # Remove the temp file, sort and output bigwig
        # Sort command with 6G maximum buffer despite 8G assigned memory
        statement = ''' rm %(temp_outfile_path)s;
        checkpoint;
        
        sort -k1,1 -k2,2n -S 6G -T %(sort_temp_dir)s %(temp_outfile2)s > %(sorted_bedgraph)s;
        checkpoint;
        
        rm %(temp_outfile2)s;
        checkpoint;
        
        bedGraphToBigWig %(sorted_bedgraph)s %(contigs)s %(temporal_output_bigwig)s 2>> %(log_file)s;
        checkpoint;
        
        rm %(sorted_bedgraph)s;
        checkpoint;
        
        mv %(temporal_output_bigwig)s %(outfile)s '''
         
        P.run()



    

@follows(generateFCBigWigs,
         generateLogPvalBigWigs,
         bdg2tdf_igv,
         index_bed_like_igv)
def generateBigWigs():
    ''' Dummy task to sync the generation of BigWigs and IGV files '''
    pass

    
    
@follows(mkdir("stats.dir"),
         getFirstFilteringStats,
         getPostDuplicationStats)
@transform(getInitialMappingStats,
           regex(".+/(.+).after_mapping.tsv"),
           add_inputs([r"stats.dir/\1.after_first_filter.tsv", 
                       r"stats.dir/\1.after_marking_dups.tsv"]),
           r"stats.dir/\1.grouped.tsv",
           r"\1")
def groupStats(infiles, outfile, sample):
    ''' Groups all the stats files corresponding to one sample together '''
    
    initial_stats = infiles[0]
    after_first_filter_stats = infiles[1][0]
    after_marking_dups_stats = infiles[1][1]
    
    pipelineAtacseq.combineStatsReports(sample,
                                        initial_stats,
                                        after_first_filter_stats,
                                        after_marking_dups_stats,
                                        outfile)
    


# ---------------------------------------------------
# Generic pipeline tasks
@follows(filter_peaks,
         merge_all_peaks_per_pooled_sample,
         generateBigWigs,
         group_stats_per_experiment,
         naive_overlap_thresholding_peaks,
         clusteringTasks,
         generateHistogramFivePrimeInsertSize,
         markDuplicatesROI,
         delete_equal_rows_binary_sample_broad_narrow_peaks_overlap_merged_all_samples_peaks, # Calculates the binary overlap of each sample's peaks with all the common peaks.
         filterShiftTagAlign)  
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
