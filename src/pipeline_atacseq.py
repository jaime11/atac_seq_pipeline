##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline template
===========================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_atacseq.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import pipelineAtacseq
import tempfile

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
# Specific pipeline tasks

@follows(mkdir("stats.dir"))
@transform("*.bam",
           regex("(.+).bam"),
           r"stats.dir/\1.after_mapping.tsv")
def getInitialMappingStats(infile, outfile):
    ''' Gets the initial mapping rate in terms of total reads '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    
    # The mapping file is sorted by coordinate, first sort by readname
    temp_file = P.snip(outfile, ".tsv") + ".bam"
    
    log_file = P.snip(outfile, ".tsv") + ".log"
    
    # Samtools creates temporary files with a certain prefix, create a temporal directory name
    samtools_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    samtools_temp_file = os.path.join(samtools_temp_dir, "temp")
    
    disk_space_file = P.snip(outfile, ".tsv") + ".txt"
        
    statement = ''' mkdir -p %(samtools_temp_dir)s;
      samtools sort -n -o %(temp_file)s -T %(samtools_temp_file)s %(infile)s 2> %(log_file)s;
      checkpoint;
      
    '''
    
    
    # Execute the statement
    P.run()
    
       
    
    # Get the stats
    pipelineAtacseq.getMappedUnmappedReads(temp_file, 
                       outfile, 
                       submit=True,
                       job_memory="6G")
    
    # Remove the temporal file
    statement = '''rm %(temp_file)s; 
    '''
    
    # Execute the statement
    P.run()



@follows(mkdir("filtered_bam.dir"))
@transform("*.bam",
           regex("(.+).bam"),
           r"filtered_bam.dir/\1.bam")
def filterOutMitochondriaMappings(infile, outfile):
    
    ''' Filter out mitochondrial mappings, keep coordinates sorting'''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Get the genome used to map the reads
    mapping_genome = PARAMS["mapping_genome"]
    
    
    log_file = P.snip(outfile, ".bam") + ".log"
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".bam") + "_temp.bam"
     
    # Samtools creates temporary files with a certain prefix
    samtools_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    statement = ''' samtools view -h %(infile)s | egrep -v chrM | 
                    samtools view -bT %(mapping_genome)s - | 
                    samtools sort - -o %(temp_file)s -T %(samtools_temp_file)s 2> %(log_file)s ;
                    checkpoint;
                    mv %(temp_file)s %(outfile)s;
                    checkpoint;
                    '''
    

    P.run()



@follows(mkdir("stats.dir"))
@transform(filterOutMitochondriaMappings,
           regex(".+/(.+).bam"),
           r"stats.dir/\1.after_chr_sel.tsv")
def getChromosomeFilteringStats(infile, outfile):
    ''' Gets the mapping rate in terms of total reads after chr filtering '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    
    # The mapping file is sorted by coordinate, first sort by readname
    temp_file = P.snip(outfile, ".tsv") + ".bam"
    
    log_file = P.snip(outfile, ".tsv") + ".log"
    
    # Samtools creates temporary files with a certain prefix, create a temporal directory name
    samtools_temp_dir = tempfile.mkdtemp(dir=tmp_dir)
    
    samtools_temp_file = os.path.join(samtools_temp_dir, "temp")
    
    disk_space_file = P.snip(outfile, ".tsv") + ".txt"
        
    statement = ''' mkdir -p %(samtools_temp_dir)s;
      samtools sort -n -o %(temp_file)s -T %(samtools_temp_file)s %(infile)s 2> %(log_file)s;
      checkpoint;
      
    '''
    
    
    # Execute the statement
    P.run()
    
       
    
    # Get the stats
    pipelineAtacseq.getMappedUnmappedReads(temp_file, 
                       outfile, 
                       submit=True,
                       job_memory="6G")
    
    # Remove the temporal file
    statement = '''rm %(temp_file)s; 
    '''
    
    # Execute the statement
    P.run()



# Assumes the files are coordinate sorted
@follows(mkdir("dedupped.dir"))
@transform(filterOutMitochondriaMappings,
           regex(".+/(.+).bam"),
           r"dedupped.dir/\1.bam")
def markDuplicates(infile, outfile):
    
    ''' Use picard to mark duplicates in BAM files (not deleted).
    The files are assumed to be coordinate sorted'''

    job_memory = "5G"

    statement = ''' MarkDuplicates
                     ASSUME_SORTED=True 
                     INPUT=%(infile)s
                     OUTPUT=%(outfile)s
                     METRICS_FILE=%(outfile)s.metrics
                   > %(outfile)s.log '''

    P.run()
    





@follows(mkdir("stats.dir"))
@transform(markDuplicates,
           regex(".+/(.+).bam"),
           r"stats.dir/\1.after_marking_dups.tsv")
def getPostDuplicationStats(infile, outfile):
     
    ''' Assuming multimapping is allowed (multiple best alignments can occur)
    Sort the reads by readname, make filterings and get the number
    unique pair mappings:
    1) Correctly mapped pairs and primary alignments only.
    2) Correctly mapped pairs and primary or secondary alignments.
    3) Correctly mapped pairs and secondary alignments only.
    get initial statistics on the reads '''
    
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
     
    sorted_bam = P.snip(outfile, ".tsv") + "_sorted.bam"
     
    log_file = P.snip(outfile, ".tsv") + ".log"
     
    bam_outfile_sec = P.snip(outfile, ".tsv") + "_sec.bam"
     
    bam_outfile_primary = P.snip(outfile, ".tsv") + "_prim.bam"
    
    
    # Samtools creates temporary files with a certain prefix
    samtools_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
     
    # First sort the bamfile
    # Then get only the primary alignments 
    statement = '''samtools sort -n -o %(sorted_bam)s -T %(samtools_temp_file)s %(infile)s 2> %(log_file)s ;
    checkpoint;
    samtools view -h -F 256 %(sorted_bam)s -o %(bam_outfile_primary)s -U %(bam_outfile_sec)s 2>> %(log_file)s;
    '''
 
    P.run()
     
    # Now see the mapped pairs and PCR duplicates in each of the 3 files
    primary_stats_file = P.snip(outfile, ".tsv") + "_primary.tsv"
    secondary_stats_file = P.snip(outfile, ".tsv") + "_secondary.tsv"
      
    # Where only primary alignments exist (1 read = 1 alignment) 
    pipelineAtacseq.getUniquelyMappedPairsNoMultimapping(bam_outfile_primary, 
                                                  primary_stats_file, 
                                                  submit=True, 
                                                  job_memory="4G")
    
    # Where multiple alignments can exist (primary + secondary)
    pipelineAtacseq.getCorrectReadPairs(sorted_bam,
                                        outfile, 
                                        submit=True,
                                        job_memory="4G")
    
    
    # Where multiple alignments can exist (secondary)
    pipelineAtacseq.getCorrectReadPairs(bam_outfile_sec,
                                        secondary_stats_file, 
                                        submit=True,
                                        job_memory="4G")






@follows(mkdir("sortedbam.dir"))
@transform(markDuplicates,
           regex(".+/(.+).bam"),
           r"sortedbam.dir/\1.bam")
def qualityFilterSort(infile, outfile):
    
    # Maybe a further pass should be performed in case some R1 or R2 are removed from
    # pairs
    '''Allow only mappings of high quality (above 20)
    allow only read pairs with reads mapped in proper pair and PCR duplicates
    (MACS2 uses them downstream).
    Supplementary mappings are allowed (for example translocations or
    inversions) are allowed.
    Sorts alignment by coordinate. '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    log_file = P.snip(outfile, ".bam") + ".log"
    
    filtered_out_alignments_file = P.snip(outfile, ".bam") + "_del_alignments.bam"
    
    # Temp file: We create a temp file to make sure the whole process goes well
    # before the actual outfile is created
    temp_file = P.snip(outfile, ".bam") + "_temp.bam"
    
    # Samtools creates temporary files with a certain prefix
    samtools_temp_file = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    
    statement = '''samtools view -q 20 -f 1027 -h -U %(filtered_out_alignments_file)s 
                -u %(infile)s | 
                samtools sort - -o %(temp_file)s -T %(samtools_temp_file)s 2> %(log_file)s ;
                checkpoint;
                mv %(temp_file)s %(outfile)s'''
    
    P.run()




# @follows(mkdir("fixed_readgroup_bams.dir"))
# @transform(qualityFilterSort,
#            formatter(".+/(?P<SAMPLE>.+?)\.(?P<REST>.+?)\.bam"), 
#            "fixed_readgroup_bams.dir/{SAMPLE[0]}.{REST[0]}.bam",
#            "{SAMPLE[0]}")
# def fixMissingReadGroups(infile, outfile, sample_name):
#     '''The quality filter of the bams can sometimes leave missing read groups.
#     This task fixes them '''
    
    

    
    
        
# @follows(mkdir("filter_bam.dir"))
# @transform(dedup,
#            regex(".+/(.+).bam"),
#            r"filter_bam.dir/\1.bam")
# def removeLowMappabilityAreas(infile, outfile):
#     ''' Removes any read pairs where any read from the pair is overlapping
#     any of the regions. If no region is specified, it just links the input file '''
#     
#     exclude_bed = PARAMS["filtering_remove_low_map"]
#     
#     if PARAMS["filtering_remove_low_map"] != "":
#     
#     
#     
#         pipelineAtacseq.filterReadMappings(infile, 
#                                        template_file, 
#                                        excluded_bed, 
#                                        outfile)
#     
#     else:
#         
#         statement = '''ln -s %(infile)s %(outfile)s'''
    
        

    


@follows(mkdir("peaks.dir"))
@transform(qualityFilterSort,
           regex(".+/(.+).bam"),
           r"peaks.dir/\1.xls")
def call_peaks(infile, outfile):
    ''' Use MACS2 to calculate peaks '''
        
    outfile_prefix = P.snip(outfile, ".xls")
    
    statement = '''macs2 callpeak -g hs --nomodel -q 0.05 -t %(infile)s -n %(outfile_prefix)s '''
    
    P.run()




@follows(mkdir("stats.dir"),
         getChromosomeFilteringStats,
         getPostDuplicationStats)
@transform(getInitialMappingStats,
           regex(".+/(.+).after_mapping.tsv"),
           add_inputs([r"stats.dir/\1.after_chr_sel.tsv", 
                       r"stats.dir/\1.after_marking_dups.tsv"]),
           r"stats.dir/\1.grouped.tsv",
           r"\1")
def groupStats(infiles, outfile, sample):
    ''' Groups all the stats files corresponding to one sample together '''
    
    initial_stats = infiles[0]
    after_chr_sel_stats = infiles[1][0]
    after_marking_dups_stats = infiles[1][1]
    
    pipelineAtacseq.combineStatsReports(sample,
                                        initial_stats,
                                        after_chr_sel_stats,
                                        after_marking_dups_stats,
                                        outfile)
    


# ---------------------------------------------------
# Generic pipeline tasks
@follows(call_peaks, groupStats)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
